{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['movieId', 'userId', 'rating', 'timestamp', 'label', 'releaseYear',\n",
       "        'movieGenre1', 'movieGenre2', 'movieGenre3', 'movieRatingCount',\n",
       "        'movieAvgRating', 'movieRatingStddev', 'userRatedMovie1',\n",
       "        'userRatedMovie2', 'userRatedMovie3', 'userRatedMovie4',\n",
       "        'userRatedMovie5', 'userRatingCount', 'userAvgReleaseYear',\n",
       "        'userReleaseYearStddev', 'userAvgRating', 'userRatingStddev',\n",
       "        'userGenre1', 'userGenre2', 'userGenre3', 'userGenre4', 'userGenre5'],\n",
       "       dtype='object'),\n",
       " 27,\n",
       " Index(['movieId', 'userId', 'rating', 'timestamp', 'label', 'releaseYear',\n",
       "        'movieGenre1', 'movieGenre2', 'movieGenre3', 'movieRatingCount',\n",
       "        'movieAvgRating', 'movieRatingStddev', 'userRatedMovie1',\n",
       "        'userRatedMovie2', 'userRatedMovie3', 'userRatedMovie4',\n",
       "        'userRatedMovie5', 'userRatingCount', 'userAvgReleaseYear',\n",
       "        'userReleaseYearStddev', 'userAvgRating', 'userRatingStddev',\n",
       "        'userGenre1', 'userGenre2', 'userGenre3', 'userGenre4', 'userGenre5'],\n",
       "       dtype='object'),\n",
       " 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../data/trainingSamples.csv\")\n",
    "df_test = pd.read_csv(\"../../data/testSamples.csv\")\n",
    "df.columns, len(df.columns), df_test.columns, len(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:799 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:530 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:630 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/utils.py:75 filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['dense_features_35/movieId_embedding/embedding_weights:0', 'dense_features_35/userId_embedding/embedding_weights:0', 'dense_features_30/movieId_embedding/embedding_weights:0', 'dense_features_31/userId_embedding/embedding_weights:0', 'dense_features_32/movieGenre1_embedding/embedding_weights:0', 'dense_features_33/userGenre1_embedding/embedding_weights:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0'].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f0dee441f9b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:799 train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:530 minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:630 apply_gradients\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    /home/wenkanw/.local/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/utils.py:75 filter_empty_gradients\n        raise ValueError(\"No gradients provided for any variable: %s.\" %\n\n    ValueError: No gradients provided for any variable: ['dense_features_35/movieId_embedding/embedding_weights:0', 'dense_features_35/userId_embedding/embedding_weights:0', 'dense_features_30/movieId_embedding/embedding_weights:0', 'dense_features_31/userId_embedding/embedding_weights:0', 'dense_features_32/movieGenre1_embedding/embedding_weights:0', 'dense_features_33/userGenre1_embedding/embedding_weights:0', 'dense_15/kernel:0', 'dense_15/bias:0', 'dense_16/kernel:0', 'dense_16/bias:0', 'dense_17/kernel:0', 'dense_17/bias:0'].\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Training samples path, change to your local path\n",
    "# training_samples_file_path = tf.keras.utils.get_file(\"trainingSamples.csv\",\n",
    "#                                                      \"file:\\\\D:\\\\MyProjects\\\\SparrowRecSys\\\\src\\\\main\\\\resources\\\\webroot\\\\sampledata\"\n",
    "#                                                      \"\\\\train.csv\")\n",
    "#                                                     #  \"\\\\trainingSamples.csv\")\n",
    "# # Test samples path, change to your local path\n",
    "# test_samples_file_path = tf.keras.utils.get_file(\"testSamples.csv\",\n",
    "#                                                  \"file:\\\\D:\\\\MyProjects\\\\SparrowRecSys\\\\src\\\\main\\\\resources\\\\webroot\\\\sampledata\"\n",
    "#                                                  \"\\\\test.csv\")\n",
    "#                                                 #  \"\\\\testSamples.csv\")\n",
    "\n",
    "\n",
    "# training_samples_file_path = \"../../data/processed_data/train.csv/part-00000-fd08a887-cd12-4275-a8c2-35dd7f171607-c000.csv\"\n",
    "# test_samples_file_path = \"../../data/processed_data/test.csv/part-00000-185a5f32-3861-4886-a6ff-3fb5638adc59-c000.csv\"\n",
    "\n",
    "training_samples_file_path = \"../../data/trainingSamples.csv\"\n",
    "test_samples_file_path = \"../../data/testSamples.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# load sample as tf dataset\n",
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=12,\n",
    "        label_name='label',\n",
    "        na_value=\"0\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# split as test dataset and training dataset\n",
    "train_dataset = get_dataset(training_samples_file_path)\n",
    "test_dataset = get_dataset(test_samples_file_path)\n",
    "\n",
    "# define input for keras model\n",
    "inputs = {\n",
    "    'movieAvgRating': tf.keras.layers.Input(name='movieAvgRating', shape=(), dtype='float32'),\n",
    "    'movieRatingStddev': tf.keras.layers.Input(name='movieRatingStddev', shape=(), dtype='float32'),\n",
    "    'rating': tf.keras.layers.Input(name='rating', shape=(), dtype='float32'),\n",
    "    'movieRatingCount': tf.keras.layers.Input(name='movieRatingCount', shape=(), dtype='int32'),\n",
    "    \n",
    "    'userAvgRating': tf.keras.layers.Input(name='userAvgRating', shape=(), dtype='float32'),\n",
    "    'userRatingStddev': tf.keras.layers.Input(name='userRatingStddev', shape=(), dtype='float32'),\n",
    "    'userRatingCount': tf.keras.layers.Input(name='userRatingCount', shape=(), dtype='int32'),\n",
    "    \n",
    "    'releaseYear': tf.keras.layers.Input(name='releaseYear', shape=(), dtype='int32'),\n",
    "    'userAvgReleaseYear': tf.keras.layers.Input(name='userAvgReleaseYear', shape=(), dtype='int32'),\n",
    "    'userReleaseYearStddev': tf.keras.layers.Input(name='userReleaseYearStddev', shape=(), dtype='int32'),\n",
    "    'timestamp': tf.keras.layers.Input(name='timestamp', shape=(), dtype='int32'),\n",
    "\n",
    "    'movieId': tf.keras.layers.Input(name='movieId', shape=(), dtype='int32'),\n",
    "    'userId': tf.keras.layers.Input(name='userId', shape=(), dtype='int32'),\n",
    "    'label': tf.keras.layers.Input(name='label', shape=(), dtype='int32'),\n",
    "    \n",
    "    'userRatedMovie1': tf.keras.layers.Input(name='userRatedMovie1', shape=(), dtype='int32'),\n",
    "    'userRatedMovie2': tf.keras.layers.Input(name='userRatedMovie2', shape=(), dtype='int32'),\n",
    "    'userRatedMovie3': tf.keras.layers.Input(name='userRatedMovie3', shape=(), dtype='int32'),\n",
    "    'userRatedMovie4': tf.keras.layers.Input(name='userRatedMovie4', shape=(), dtype='int32'),\n",
    "    'userRatedMovie5': tf.keras.layers.Input(name='userRatedMovie5', shape=(), dtype='int32'),\n",
    "\n",
    "    'userGenre1': tf.keras.layers.Input(name='userGenre1', shape=(), dtype='string'),\n",
    "    'userGenre2': tf.keras.layers.Input(name='userGenre2', shape=(), dtype='string'),\n",
    "    'userGenre3': tf.keras.layers.Input(name='userGenre3', shape=(), dtype='string'),\n",
    "    'userGenre4': tf.keras.layers.Input(name='userGenre4', shape=(), dtype='string'),\n",
    "    'userGenre5': tf.keras.layers.Input(name='userGenre5', shape=(), dtype='string'),\n",
    "    \n",
    "    'movieGenre1': tf.keras.layers.Input(name='movieGenre1', shape=(), dtype='string'),\n",
    "    'movieGenre2': tf.keras.layers.Input(name='movieGenre2', shape=(), dtype='string'),\n",
    "    'movieGenre3': tf.keras.layers.Input(name='movieGenre3', shape=(), dtype='string'),\n",
    "}\n",
    "\n",
    "# movie id embedding feature\n",
    "movie_col = tf.feature_column.categorical_column_with_identity(key='movieId', num_buckets=1001)\n",
    "movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)\n",
    "movie_ind_col = tf.feature_column.indicator_column(movie_col) # movid id indicator columns\n",
    "\n",
    "# user id embedding feature\n",
    "user_col = tf.feature_column.categorical_column_with_identity(key='userId', num_buckets=30001)\n",
    "user_emb_col = tf.feature_column.embedding_column(user_col, 10)\n",
    "user_ind_col = tf.feature_column.indicator_column(user_col) # user id indicator columns\n",
    "\n",
    "# genre features vocabulary\n",
    "genre_vocab = ['Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy', 'Western', 'Documentary',\n",
    "               'Sci-Fi', 'Drama', 'Thriller',\n",
    "               'Crime', 'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical']\n",
    "# user genre embedding feature\n",
    "user_genre_col = tf.feature_column.categorical_column_with_vocabulary_list(key=\"userGenre1\",\n",
    "                                                                           vocabulary_list=genre_vocab)\n",
    "user_genre_emb_col = tf.feature_column.embedding_column(user_genre_col, 10)\n",
    "user_genre_ind_col = tf.feature_column.indicator_column(user_genre_col) # user genre indicator columns\n",
    "# item genre embedding feature\n",
    "item_genre_col = tf.feature_column.categorical_column_with_vocabulary_list(key=\"movieGenre1\",\n",
    "                                                                           vocabulary_list=genre_vocab)\n",
    "item_genre_emb_col = tf.feature_column.embedding_column(item_genre_col, 10)\n",
    "item_genre_ind_col = tf.feature_column.indicator_column(item_genre_col) # item genre indicator columns\n",
    "\n",
    "# fm first-order term columns: without embedding and concatenate to the output layer directly\n",
    "fm_first_order_columns = [movie_ind_col, user_ind_col, user_genre_ind_col, item_genre_ind_col]\n",
    "\n",
    "deep_feature_columns = [tf.feature_column.numeric_column('releaseYear'),\n",
    "                        tf.feature_column.numeric_column('movieRatingCount'),\n",
    "                        tf.feature_column.numeric_column('movieAvgRating'),\n",
    "                        tf.feature_column.numeric_column('movieRatingStddev'),\n",
    "                        tf.feature_column.numeric_column('userRatingCount'),\n",
    "                        tf.feature_column.numeric_column('userAvgRating'),\n",
    "                        tf.feature_column.numeric_column('userRatingStddev'),\n",
    "                        movie_emb_col,\n",
    "                        user_emb_col]\n",
    "\n",
    "item_emb_layer = tf.keras.layers.DenseFeatures([movie_emb_col])(inputs)\n",
    "user_emb_layer = tf.keras.layers.DenseFeatures([user_emb_col])(inputs)\n",
    "# item_genre_emb_layer = tf.keras.layers.DenseFeatures([item_genre_emb_col])(inputs)\n",
    "# user_genre_emb_layer = tf.keras.layers.DenseFeatures([user_genre_emb_col])(inputs)\n",
    "\n",
    "item_genre_emb_layer = tf.keras.layers.DenseFeatures([item_genre_emb_col])(inputs)\n",
    "user_genre_emb_layer = tf.keras.layers.DenseFeatures([user_genre_emb_col])(inputs)\n",
    "\n",
    "\n",
    "# The first-order term in the FM layer\n",
    "fm_first_order_layer = tf.keras.layers.DenseFeatures(fm_first_order_columns)(inputs)\n",
    "\n",
    "# FM part, cross different categorical feature embeddings\n",
    "product_layer_item_user = tf.keras.layers.Dot(axes=1)([item_emb_layer, user_emb_layer])\n",
    "product_layer_item_genre_user_genre = tf.keras.layers.Dot(axes=1)([item_genre_emb_layer, user_genre_emb_layer])\n",
    "product_layer_item_genre_user = tf.keras.layers.Dot(axes=1)([item_genre_emb_layer, user_emb_layer])\n",
    "product_layer_user_genre_item = tf.keras.layers.Dot(axes=1)([item_emb_layer, user_genre_emb_layer])\n",
    "\n",
    "# deep part, MLP to generalize all input features\n",
    "deep = tf.keras.layers.DenseFeatures(deep_feature_columns)(inputs)\n",
    "deep = tf.keras.layers.Dense(64, activation='relu')(deep)\n",
    "deep = tf.keras.layers.Dense(64, activation='relu')(deep)\n",
    "\n",
    "# concatenate fm part and deep part\n",
    "concat_layer = tf.keras.layers.concatenate([fm_first_order_layer, product_layer_item_user, product_layer_item_genre_user_genre,\n",
    "                                            product_layer_item_genre_user, product_layer_user_genre_item, deep], axis=1)\n",
    "\n",
    "# concat_layer = deep\n",
    "\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(concat_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs, output_layer)\n",
    "# compile the model, set loss function, optimizer and evaluation metrics\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(curve='ROC'), tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "# train the model\n",
    "model.fit(next(iter(train_dataset)), epochs=5)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_accuracy, test_roc_auc, test_pr_auc = model.evaluate(test_dataset)\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}, Test ROC AUC {}, Test PR AUC {}'.format(test_loss, test_accuracy,\n",
    "                                                                                   test_roc_auc, test_pr_auc))\n",
    "\n",
    "# print some predict results\n",
    "predictions = model.predict(test_dataset)\n",
    "for prediction, goodRating in zip(predictions[:12], list(test_dataset)[0][1][:12]):\n",
    "    print(\"Predicted good rating: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual rating label: \",\n",
    "          (\"Good Rating\" if bool(goodRating) else \"Bad Rating\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide&Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7403/7403 [==============================] - 81s 11ms/step - loss: 0.7434 - accuracy: 0.6061 - auc_16: 0.6223 - auc_17: 0.6553\n",
      "Epoch 2/5\n",
      "7403/7403 [==============================] - 79s 11ms/step - loss: 0.6082 - accuracy: 0.6757 - auc_16: 0.7276 - auc_17: 0.7520\n",
      "Epoch 3/5\n",
      "7403/7403 [==============================] - 79s 11ms/step - loss: 0.5583 - accuracy: 0.7174 - auc_16: 0.7809 - auc_17: 0.8009\n",
      "Epoch 4/5\n",
      "7403/7403 [==============================] - 79s 11ms/step - loss: 0.5196 - accuracy: 0.7472 - auc_16: 0.8149 - auc_17: 0.8313\n",
      "Epoch 5/5\n",
      "7403/7403 [==============================] - 79s 11ms/step - loss: 0.4931 - accuracy: 0.7657 - auc_16: 0.8354 - auc_17: 0.8508\n",
      "1870/1870 [==============================] - 15s 7ms/step - loss: 0.6133 - accuracy: 0.6927 - auc_16: 0.7525 - auc_17: 0.7732\n",
      "\n",
      "\n",
      "Test Loss 0.6133310198783875, Test Accuracy 0.6926916241645813, Test ROC AUC 0.7524797916412354, Test PR AUC 0.7732200622558594\n",
      "Predicted good rating: 39.42%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 79.23%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 41.73%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 16.13%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 24.97%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 1.61%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 41.74%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 82.20%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 50.81%  | Actual rating label:  Bad Rating\n",
      "Predicted good rating: 42.24%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 71.45%  | Actual rating label:  Good Rating\n",
      "Predicted good rating: 74.74%  | Actual rating label:  Good Rating\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "training_samples_file_path = \"../../data/trainingSamples.csv\"\n",
    "test_samples_file_path = \"../../data/testSamples.csv\"\n",
    "# training_samples_file_path = tf.keras.utils.get_file(\"trainingSamples.csv\",\n",
    "#                                                      \"file:\\\\D:\\\\MyProjects\\\\SparrowRecSys\\\\src\\\\main\\\\resources\\\\webroot\\\\sampledata\"\n",
    "#                                                      \"\\\\train.csv\")\n",
    "#                                                     #  \"\\\\trainingSamples.csv\")\n",
    "# # Test samples path, change to your local path\n",
    "# test_samples_file_path = tf.keras.utils.get_file(\"testSamples.csv\",\n",
    "#                                                  \"file:\\\\D:\\\\MyProjects\\\\SparrowRecSys\\\\src\\\\main\\\\resources\\\\webroot\\\\sampledata\"\n",
    "#                                                  \"\\\\test.csv\")\n",
    "# load sample as tf dataset\n",
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=12,\n",
    "        label_name='label',\n",
    "        na_value=\"0\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# split as test dataset and training dataset\n",
    "train_dataset = get_dataset(training_samples_file_path)\n",
    "test_dataset = get_dataset(test_samples_file_path)\n",
    "\n",
    "# genre features vocabulary\n",
    "genre_vocab = ['Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy', 'Western', 'Documentary',\n",
    "               'Sci-Fi', 'Drama', 'Thriller',\n",
    "               'Crime', 'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical']\n",
    "\n",
    "GENRE_FEATURES = {\n",
    "    'userGenre1': genre_vocab,\n",
    "    'userGenre2': genre_vocab,\n",
    "    'userGenre3': genre_vocab,\n",
    "    'userGenre4': genre_vocab,\n",
    "    'userGenre5': genre_vocab,\n",
    "    'movieGenre1': genre_vocab,\n",
    "    'movieGenre2': genre_vocab,\n",
    "    'movieGenre3': genre_vocab,\n",
    "    \n",
    "}\n",
    "\n",
    "# all categorical features\n",
    "categorical_columns = []\n",
    "for feature, vocab in GENRE_FEATURES.items():\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "    emb_col = tf.feature_column.embedding_column(cat_col, 10)\n",
    "    categorical_columns.append(emb_col)\n",
    "    \n",
    "# movie id embedding feature\n",
    "movie_col = tf.feature_column.categorical_column_with_identity(key='movieId', num_buckets=1001)\n",
    "movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)\n",
    "categorical_columns.append(movie_emb_col)\n",
    "\n",
    "# user id embedding feature\n",
    "user_col = tf.feature_column.categorical_column_with_identity(key='userId', num_buckets=30001)\n",
    "user_emb_col = tf.feature_column.embedding_column(user_col, 10)\n",
    "categorical_columns.append(user_emb_col)\n",
    "\n",
    "# all numerical features\n",
    "numerical_columns = [tf.feature_column.numeric_column('releaseYear'),\n",
    "                     tf.feature_column.numeric_column('movieRatingCount'),\n",
    "                     tf.feature_column.numeric_column('movieAvgRating'),\n",
    "                     tf.feature_column.numeric_column('movieRatingStddev'),\n",
    "                     tf.feature_column.numeric_column('userRatingCount'),\n",
    "                     tf.feature_column.numeric_column('userAvgRating'),\n",
    "                     tf.feature_column.numeric_column('userRatingStddev')]\n",
    "\n",
    "# cross feature between current movie and user historical movie\n",
    "rated_movie = tf.feature_column.categorical_column_with_identity(key='userRatedMovie1', num_buckets=1001)\n",
    "crossed_feature = tf.feature_column.indicator_column(tf.feature_column.crossed_column([movie_col, rated_movie], 10000))\n",
    "\n",
    "# define input for keras model\n",
    "inputs =  {\n",
    "    'movieAvgRating': tf.keras.layers.Input(name='movieAvgRating', shape=(), dtype='float32'),\n",
    "    'movieRatingStddev': tf.keras.layers.Input(name='movieRatingStddev', shape=(), dtype='float32'),\n",
    "    'rating': tf.keras.layers.Input(name='rating', shape=(), dtype='float32'),\n",
    "    'movieRatingCount': tf.keras.layers.Input(name='movieRatingCount', shape=(), dtype='int32'),\n",
    "    \n",
    "    'userAvgRating': tf.keras.layers.Input(name='userAvgRating', shape=(), dtype='float32'),\n",
    "    'userRatingStddev': tf.keras.layers.Input(name='userRatingStddev', shape=(), dtype='float32'),\n",
    "    'userRatingCount': tf.keras.layers.Input(name='userRatingCount', shape=(), dtype='int32'),\n",
    "    \n",
    "    'releaseYear': tf.keras.layers.Input(name='releaseYear', shape=(), dtype='int32'),\n",
    "    'userAvgReleaseYear': tf.keras.layers.Input(name='userAvgReleaseYear', shape=(), dtype='int32'),\n",
    "    'userReleaseYearStddev': tf.keras.layers.Input(name='userReleaseYearStddev', shape=(), dtype='int32'),\n",
    "    'timestamp': tf.keras.layers.Input(name='timestamp', shape=(), dtype='int32'),\n",
    "\n",
    "    'movieId': tf.keras.layers.Input(name='movieId', shape=(), dtype='int32'),\n",
    "    'userId': tf.keras.layers.Input(name='userId', shape=(), dtype='int32'),\n",
    "    \n",
    "    'userRatedMovie1': tf.keras.layers.Input(name='userRatedMovie1', shape=(), dtype='int32'),\n",
    "    'userRatedMovie2': tf.keras.layers.Input(name='userRatedMovie2', shape=(), dtype='int32'),\n",
    "    'userRatedMovie3': tf.keras.layers.Input(name='userRatedMovie3', shape=(), dtype='int32'),\n",
    "    'userRatedMovie4': tf.keras.layers.Input(name='userRatedMovie4', shape=(), dtype='int32'),\n",
    "    'userRatedMovie5': tf.keras.layers.Input(name='userRatedMovie5', shape=(), dtype='int32'),\n",
    "\n",
    "    'userGenre1': tf.keras.layers.Input(name='userGenre1', shape=(), dtype='string'),\n",
    "    'userGenre2': tf.keras.layers.Input(name='userGenre2', shape=(), dtype='string'),\n",
    "    'userGenre3': tf.keras.layers.Input(name='userGenre3', shape=(), dtype='string'),\n",
    "    'userGenre4': tf.keras.layers.Input(name='userGenre4', shape=(), dtype='string'),\n",
    "    'userGenre5': tf.keras.layers.Input(name='userGenre5', shape=(), dtype='string'),\n",
    "    \n",
    "    'movieGenre1': tf.keras.layers.Input(name='movieGenre1', shape=(), dtype='string'),\n",
    "    'movieGenre2': tf.keras.layers.Input(name='movieGenre2', shape=(), dtype='string'),\n",
    "    'movieGenre3': tf.keras.layers.Input(name='movieGenre3', shape=(), dtype='string'),\n",
    "}\n",
    "\n",
    "\n",
    "# wide and deep model architecture\n",
    "# deep part for all input features\n",
    "deep = tf.keras.layers.DenseFeatures(numerical_columns + categorical_columns)(inputs)\n",
    "deep = tf.keras.layers.Dense(128, activation='relu')(deep)\n",
    "deep = tf.keras.layers.Dense(128, activation='relu')(deep)\n",
    "# wide part for cross feature\n",
    "wide = tf.keras.layers.DenseFeatures(crossed_feature)(inputs)\n",
    "both = tf.keras.layers.concatenate([deep, wide])\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(both)\n",
    "model = tf.keras.Model(inputs, output_layer)\n",
    "\n",
    "# compile the model, set loss function, optimizer and evaluation metrics\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(curve='ROC'), tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_dataset, epochs=5)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_accuracy, test_roc_auc, test_pr_auc = model.evaluate(test_dataset)\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}, Test ROC AUC {}, Test PR AUC {}'.format(test_loss, test_accuracy,\n",
    "                                                                                   test_roc_auc, test_pr_auc))\n",
    "\n",
    "# print some predict results\n",
    "predictions = model.predict(test_dataset)\n",
    "for prediction, goodRating in zip(predictions[:12], list(test_dataset)[0][1][:12]):\n",
    "    print(\"Predicted good rating: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual rating label: \",\n",
    "          (\"Good Rating\" if bool(goodRating) else \"Bad Rating\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "training_samples_file_path = \"../../data/trainingSamples.csv\"\n",
    "test_samples_file_path = \"../../data/testSamples.csv\"\n",
    "# training_samples_file_path = tf.keras.utils.get_file(\"trainingSamples.csv\",\n",
    "#                                                      \"file:\\\\D:\\\\MyProjects\\\\SparrowRecSys\\\\src\\\\main\\\\resources\\\\webroot\\\\sampledata\"\n",
    "#                                                      \"\\\\train.csv\")\n",
    "#                                                     #  \"\\\\trainingSamples.csv\")\n",
    "# # Test samples path, change to your local path\n",
    "# test_samples_file_path = tf.keras.utils.get_file(\"testSamples.csv\",\n",
    "#                                                  \"file:\\\\D:\\\\MyProjects\\\\SparrowRecSys\\\\src\\\\main\\\\resources\\\\webroot\\\\sampledata\"\n",
    "#                                                  \"\\\\test.csv\")\n",
    "\n",
    "\n",
    "# load sample as tf dataset\n",
    "def get_dataset(file_path):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path,\n",
    "        batch_size=12,\n",
    "        label_name='label',\n",
    "        na_value=\"0\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# split as test dataset and training dataset\n",
    "train_dataset = get_dataset(training_samples_file_path)\n",
    "test_dataset = get_dataset(test_samples_file_path)\n",
    "\n",
    "# genre features vocabulary\n",
    "genre_vocab = ['Film-Noir', 'Action', 'Adventure', 'Horror', 'Romance', 'War', 'Comedy', 'Western', 'Documentary',\n",
    "               'Sci-Fi', 'Drama', 'Thriller',\n",
    "               'Crime', 'Fantasy', 'Animation', 'IMAX', 'Mystery', 'Children', 'Musical']\n",
    "\n",
    "GENRE_FEATURES = {\n",
    "    'userGenre1': genre_vocab,\n",
    "    'userGenre2': genre_vocab,\n",
    "    'userGenre3': genre_vocab,\n",
    "    'userGenre4': genre_vocab,\n",
    "    'userGenre5': genre_vocab,\n",
    "    'movieGenre1': genre_vocab,\n",
    "    'movieGenre2': genre_vocab,\n",
    "    'movieGenre3': genre_vocab,\n",
    "    'userRatedMovie1': genre_vocab,\n",
    "    'userRatedMovie2': genre_vocab,\n",
    "    'userRatedMovie3': genre_vocab,\n",
    "    'userRatedMovie4': genre_vocab,\n",
    "    'userRatedMovie5': genre_vocab,\n",
    "    \n",
    "}\n",
    "\n",
    "# all categorical features\n",
    "categorical_columns = []\n",
    "for feature, vocab in GENRE_FEATURES.items():\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "    emb_col = tf.feature_column.embedding_column(cat_col, 10)\n",
    "    categorical_columns.append(emb_col)\n",
    "    \n",
    "# movie id embedding feature\n",
    "movie_col = tf.feature_column.categorical_column_with_identity(key='movieId', num_buckets=1001)\n",
    "movie_emb_col = tf.feature_column.embedding_column(movie_col, 10)\n",
    "categorical_columns.append(movie_emb_col)\n",
    "\n",
    "# user id embedding feature\n",
    "user_col = tf.feature_column.categorical_column_with_identity(key='userId', num_buckets=30001)\n",
    "user_emb_col = tf.feature_column.embedding_column(user_col, 10)\n",
    "categorical_columns.append(user_emb_col)\n",
    "\n",
    "# all numerical features\n",
    "numerical_columns = [tf.feature_column.numeric_column('releaseYear'),\n",
    "                     tf.feature_column.numeric_column('movieRatingCount'),\n",
    "                     tf.feature_column.numeric_column('movieAvgRating'),\n",
    "                     tf.feature_column.numeric_column('movieRatingStddev'),\n",
    "                     tf.feature_column.numeric_column('userRatingCount'),\n",
    "                     tf.feature_column.numeric_column('userAvgRating'),\n",
    "                     tf.feature_column.numeric_column('userRatingStddev')]\n",
    "\n",
    "# cross feature between current movie and user historical movie\n",
    "rated_movie = tf.feature_column.categorical_column_with_identity(key='userRatedMovie1', num_buckets=1001)\n",
    "crossed_feature = tf.feature_column.indicator_column(tf.feature_column.crossed_column([movie_col, rated_movie], 10000))\n",
    "\n",
    "# define input for keras model\n",
    "inputs  = {\n",
    "    'movieAvgRating': tf.keras.layers.Input(name='movieAvgRating', shape=(), dtype='float32'),\n",
    "    'movieRatingStddev': tf.keras.layers.Input(name='movieRatingStddev', shape=(), dtype='float32'),\n",
    "    'rating': tf.keras.layers.Input(name='rating', shape=(), dtype='float32'),\n",
    "    'movieRatingCount': tf.keras.layers.Input(name='movieRatingCount', shape=(), dtype='int32'),\n",
    "    \n",
    "    'userAvgRating': tf.keras.layers.Input(name='userAvgRating', shape=(), dtype='float32'),\n",
    "    'userRatingStddev': tf.keras.layers.Input(name='userRatingStddev', shape=(), dtype='float32'),\n",
    "    'userRatingCount': tf.keras.layers.Input(name='userRatingCount', shape=(), dtype='int32'),\n",
    "    \n",
    "    'releaseYear': tf.keras.layers.Input(name='releaseYear', shape=(), dtype='int32'),\n",
    "    'userAvgReleaseYear': tf.keras.layers.Input(name='userAvgReleaseYear', shape=(), dtype='int32'),\n",
    "    'userReleaseYearStddev': tf.keras.layers.Input(name='userReleaseYearStddev', shape=(), dtype='int32'),\n",
    "    'timestamp': tf.keras.layers.Input(name='timestamp', shape=(), dtype='int32'),\n",
    "\n",
    "    'movieId': tf.keras.layers.Input(name='movieId', shape=(), dtype='int32'),\n",
    "    'userId': tf.keras.layers.Input(name='userId', shape=(), dtype='int32'),\n",
    "    \n",
    "    'userRatedMovie1': tf.keras.layers.Input(name='userRatedMovie1', shape=(), dtype='int32'),\n",
    "    'userRatedMovie2': tf.keras.layers.Input(name='userRatedMovie2', shape=(), dtype='int32'),\n",
    "    'userRatedMovie3': tf.keras.layers.Input(name='userRatedMovie3', shape=(), dtype='int32'),\n",
    "    'userRatedMovie4': tf.keras.layers.Input(name='userRatedMovie4', shape=(), dtype='int32'),\n",
    "    'userRatedMovie5': tf.keras.layers.Input(name='userRatedMovie5', shape=(), dtype='int32'),\n",
    "\n",
    "    'userGenre1': tf.keras.layers.Input(name='userGenre1', shape=(), dtype='string'),\n",
    "    'userGenre2': tf.keras.layers.Input(name='userGenre2', shape=(), dtype='string'),\n",
    "    'userGenre3': tf.keras.layers.Input(name='userGenre3', shape=(), dtype='string'),\n",
    "    'userGenre4': tf.keras.layers.Input(name='userGenre4', shape=(), dtype='string'),\n",
    "    'userGenre5': tf.keras.layers.Input(name='userGenre5', shape=(), dtype='string'),\n",
    "    \n",
    "    'movieGenre1': tf.keras.layers.Input(name='movieGenre1', shape=(), dtype='string'),\n",
    "    'movieGenre2': tf.keras.layers.Input(name='movieGenre2', shape=(), dtype='string'),\n",
    "    'movieGenre3': tf.keras.layers.Input(name='movieGenre3', shape=(), dtype='string'),\n",
    "}\n",
    "\n",
    "\n",
    "# wide and deep model architecture\n",
    "# deep part for all input features\n",
    "deep = tf.keras.layers.DenseFeatures(numerical_columns + categorical_columns)(inputs)\n",
    "deep = tf.keras.layers.Dense(128, activation='relu')(deep)\n",
    "deep = tf.keras.layers.Dense(128, activation='relu')(deep)\n",
    "# wide part for cross feature\n",
    "wide = tf.keras.layers.DenseFeatures(crossed_feature)(inputs)\n",
    "both = tf.keras.layers.concatenate([deep, wide])\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(both)\n",
    "model = tf.keras.Model(inputs, output_layer)\n",
    "\n",
    "# compile the model, set loss function, optimizer and evaluation metrics\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(curve='ROC'), tf.keras.metrics.AUC(curve='PR')])\n",
    "\n",
    "# train the model\n",
    "model.fit(train_dataset, epochs=5)\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_accuracy, test_roc_auc, test_pr_auc = model.evaluate(test_dataset)\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}, Test ROC AUC {}, Test PR AUC {}'.format(test_loss, test_accuracy,\n",
    "                                                                                   test_roc_auc, test_pr_auc))\n",
    "\n",
    "# print some predict results\n",
    "predictions = model.predict(test_dataset)\n",
    "for prediction, goodRating in zip(predictions[:12], list(test_dataset)[0][1][:12]):\n",
    "    print(\"Predicted good rating: {:.2%}\".format(prediction[0]),\n",
    "          \" | Actual rating label: \",\n",
    "          (\"Good Rating\" if bool(goodRating) else \"Bad Rating\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
