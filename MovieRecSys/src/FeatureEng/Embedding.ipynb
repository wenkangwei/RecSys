{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "# from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ItemSquence...\n",
      "325 0.942238450050354\n",
      "255 0.9304288029670715\n",
      "330 0.930080771446228\n",
      "606 0.9175326228141785\n",
      "93 0.9100475311279297\n",
      "267 0.9068793058395386\n",
      "415 0.9025557637214661\n",
      "437 0.9016014337539673\n",
      "275 0.9014120101928711\n",
      "366 0.8982865214347839\n",
      "505 0.8923134207725525\n",
      "413 0.890252947807312\n",
      "433 0.8874485492706299\n",
      "177 0.8846246600151062\n",
      "240 0.8830882906913757\n",
      "174 0.8828160762786865\n",
      "386 0.8783508539199829\n",
      "328 0.8764989376068115\n",
      "312 0.8761969208717346\n",
      "476 0.8732856512069702\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def getItemSeqs(spark, samplesRating):\n",
    "    \"\"\"\n",
    "    extract item sequences for each user from dataframe\n",
    "    1. for each user, collect the corresponding visited movies and timestamp into a list\n",
    "    2. use UDF to process movie list and timestamp list to sort the movie sequence for each user\n",
    "    3. join the movie list to get a string for each user\n",
    "    \"\"\"\n",
    "    def sortF(movie_list, timestamp_list):\n",
    "        \"\"\"\n",
    "        sort by time and return the corresponding movie sequence\n",
    "        eg:\n",
    "            input: movie_list:[1,2,3]\n",
    "                   timestamp_list:[1112486027,1212546032,1012486033]\n",
    "            return [3,1,2]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        # concat timestamp with movie id\n",
    "        for m, t in zip(movie_list, timestamp_list):\n",
    "            pairs.append((m, t))\n",
    "        # sort by time\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])\n",
    "        return [x[0] for x in pairs]\n",
    "    \n",
    "    \n",
    "    sortUDF = udf(sortF, ArrayType(StringType()))\n",
    "    \n",
    "    # rating data\n",
    "    # ratingSamples.show(5)\n",
    "    # ratingSamples.printSchema()\n",
    "    userSequence = samplesRating.where(F.col(\"rating\") > 3) \\\n",
    "                    .groupBy(\"userId\")\\\n",
    "                    .agg(sortUDF(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias(\"movieIds\"))\\\n",
    "                    .withColumn(\"movieIdStr\", F.array_join(F.col(\"movieIds\"), \" \"))\n",
    "    seq = userSequence.select(\"movieIdStr\").rdd.map(lambda x : x[0].split(\" \"))\n",
    "    \n",
    "    return seq\n",
    "\n",
    "\n",
    "\n",
    "def embeddingLSH(spark, movieEmbMap):\n",
    "    \"\"\"\n",
    "    Local sensitive hashing using bucketedRandomProjection\n",
    "    \"\"\"\n",
    "    movieEmbSeq = []\n",
    "    for key, embedding_list in movieEmbMap.items():\n",
    "        embedding_list = [np.float64(embedding) for embedding in embedding_list]\n",
    "        movieEmbSeq.append((key, Vectors.dense(embedding_list)))\n",
    "    movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\")\n",
    "    bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1,\n",
    "                                                      numHashTables=3)\n",
    "    bucketModel = bucketProjectionLSH.fit(movieEmbDF)\n",
    "    embBucketResult = bucketModel.transform(movieEmbDF)\n",
    "    print(\"movieId, emb, bucketId schema:\")\n",
    "    embBucketResult.printSchema()\n",
    "    print(\"movieId, emb, bucketId data result:\")\n",
    "    embBucketResult.show(10, truncate=False)\n",
    "    print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\")\n",
    "    sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237)\n",
    "    bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "def getTransitionMatrix(item_seq):\n",
    "    \"\"\"\n",
    "    build graph and transition matrix based on input item sequences \n",
    "    \"\"\"\n",
    "    def generate_pair(ls):\n",
    "        \"\"\"\n",
    "        use a sliding window with size of 2 to generate item pairs\n",
    "        input: ls =  list of items \n",
    "        output: list of pair \n",
    "        example:\n",
    "            input: [86, 90, 11, 100,]\n",
    "            output: [[86,90], [90, 11], [11,100]]\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        prev = None\n",
    "        for i in range(len(ls)):\n",
    "            if i >0:\n",
    "                res.append([ls[i-1],ls[i]])\n",
    "                \n",
    "    return res\n",
    "\n",
    "def randomWalk(trans_mat, walk_length):\n",
    "    \"\"\"\n",
    "    generate one random walk sequence based on transition matrix\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def generateItemSeqs(trans_mat, num_seq=20000, emb_length = 10  ):\n",
    "    \"\"\"\n",
    "    use random walk to generate multiple item sequences\n",
    "    \"\"\"\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# def trainItem2vec(spark, samples, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "#     word2vec = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "#     model = word2vec.fit(samples)\n",
    "#     synonyms = model.findSynonyms(\"158\", 20)\n",
    "#     for synonym, cosineSimilarity in synonyms:\n",
    "#         print(synonym, cosineSimilarity)\n",
    "#     embOutputDir = '/'.join(embOutputPath.split('/')[:-1])\n",
    "#     if not os.path.exists(embOutputDir):\n",
    "#         os.makedirs(embOutputDir)\n",
    "#     with open(embOutputPath, 'w') as f:\n",
    "#         for movie_id in model.getVectors():\n",
    "#             vectors = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "#             f.write(movie_id + \":\" + vectors + \"\\n\")\n",
    "#     embeddingLSH(spark, model.getVectors())\n",
    "#     return model\n",
    "\n",
    "\n",
    "def trainItem2Vec(item_seqs, emb_length, output_path, save_to_redis, redis_keyprefix):\n",
    "    \"\"\"\n",
    "    use Word2Vec to train item embedding\n",
    "    input:\n",
    "        - item_seqs: RDD pipeline instance, rather than dataframe\n",
    "    Note:  \n",
    "    - Word2Vec from mllib is a function that take RDD pipeline as input.\n",
    "    - Word2Vec from ml is a function that take Dataframe as input \n",
    "    \n",
    "    \"\"\"\n",
    "    # train word2Vec\n",
    "#     w2v = Word2Vec(vectorSize=emb_length, windowSize = 5, maxIter = 10, seed=42)\n",
    "    w2v = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "    model = w2v.fit(item_seqs)\n",
    "    # test word2vec\n",
    "    synonyms = model.findSynonyms(\"157\", 20)\n",
    "    for synonym, cos_similarity in synonyms:\n",
    "        print(synonym, cos_similarity)\n",
    "    \n",
    "    # save word2Vec to input path \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        for movie_id in model.getVectors():\n",
    "            # convert vector to string type and store it\n",
    "            vector = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "            pair = movie_id + \":\" + vector + \"\\n\"\n",
    "            fp.write(pair)\n",
    "    return model\n",
    "\n",
    "\n",
    "def DeepWalk(spark, item_seq, walk_length, num_walk, output_file, save_to_redis=False, redis_key_prefix=None):\n",
    "    \"\"\"\n",
    "    use DeepWalk to generate graph embeddings\n",
    "    input:\n",
    "        - item_seq: RDD based sequence of item visited by a user\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # construct probability graph\n",
    "    trans_mat = getTransitionMatrix(item_seq)\n",
    "    \n",
    "    # generate sequence samples randomly\n",
    "    samples = generateItemSeqs(trans_mat, num_seq=20000, emb_length = 10 )\n",
    "    # train item2Vec\n",
    "    graphEmb = trainItem2Vec(spark, samples)\n",
    "    \n",
    "    return graphEmb\n",
    "\n",
    "def getUserEmb():\n",
    "    \"\"\"\n",
    "    generate user embedding based on item embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    # Change to your own filepath\n",
    "    file_path = '../../data/'\n",
    "    rawSampleDataPath = file_path + \"ratings.csv\"\n",
    "    embLength = 10\n",
    "    print(\"Process ItemSquence...\")\n",
    "    samplesRating = spark.read.csv(rawSampleDataPath, header = True)\n",
    "    item_seqs = getItemSeqs(spark, samplesRating)\n",
    "    #print(samples)\n",
    "    \n",
    "    trainItem2Vec(item_seqs, emb_length=10, output_path=file_path+\"modeldata/itemGraphEmb.csv\", save_to_redis=False, redis_keyprefix=None)\n",
    "#     print(\"Train Item2Vec...\")\n",
    "#     model = trainItem2vec(spark, samples, embLength,\n",
    "#                           embOutputPath=file_path + \"modeldata/item2vecEmb.csv\", saveToRedis=False,\n",
    "#                           redisKeyPrefix=\"i2vEmb\")\n",
    "#     print(\"Train graph Embedding...\")\n",
    "#     graphEmb(samples, spark, embLength, embOutputFilename=file_path + \"modeldata/itemGraphEmb.csv\",\n",
    "#              saveToRedis=True, redisKeyPrefix=\"graphEmb\")\n",
    "#     print(\"Train User Embedding...\")\n",
    "#     generateUserEmb(spark, rawSampleDataPath, model, embLength,\n",
    "#                     embOutputPath=file_path + \"modeldata/userEmb.csv\", saveToRedis=False,\n",
    "#                     redisKeyPrefix=\"uEmb\")\n",
    "    print(\"Done!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UdfFunction:\n",
    "    @staticmethod\n",
    "    def sortF(movie_list, timestamp_list):\n",
    "        \"\"\"\n",
    "        sort by time and return the corresponding movie sequence\n",
    "        eg:\n",
    "            input: movie_list:[1,2,3]\n",
    "                   timestamp_list:[1112486027,1212546032,1012486033]\n",
    "            return [3,1,2]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for m, t in zip(movie_list, timestamp_list):\n",
    "            pairs.append((m, t))\n",
    "        # sort by time\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])\n",
    "        return [x[0] for x in pairs]\n",
    "\n",
    "\n",
    "def processItemSequence(spark, rawSampleDataPath):\n",
    "    # rating data\n",
    "    ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "    # ratingSamples.show(5)\n",
    "    # ratingSamples.printSchema()\n",
    "    sortUdf = udf(UdfFunction.sortF, ArrayType(StringType()))\n",
    "    userSeq = ratingSamples \\\n",
    "        .where(F.col(\"rating\") >= 3.5) \\\n",
    "        .groupBy(\"userId\") \\\n",
    "        .agg(sortUdf(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias('movieIds')) \\\n",
    "        .withColumn(\"movieIdStr\", array_join(F.col(\"movieIds\"), \" \"))\n",
    "    # userSeq.select(\"userId\", \"movieIdStr\").show(10, truncate = False)\n",
    "    return userSeq.select('movieIdStr').rdd.map(lambda x: x[0].split(' '))\n",
    "\n",
    "\n",
    "def embeddingLSH(spark, movieEmbMap):\n",
    "    movieEmbSeq = []\n",
    "    for key, embedding_list in movieEmbMap.items():\n",
    "        embedding_list = [np.float64(embedding) for embedding in embedding_list]\n",
    "        movieEmbSeq.append((key, Vectors.dense(embedding_list)))\n",
    "    movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\")\n",
    "    bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1,\n",
    "                                                      numHashTables=3)\n",
    "    bucketModel = bucketProjectionLSH.fit(movieEmbDF)\n",
    "    embBucketResult = bucketModel.transform(movieEmbDF)\n",
    "    print(\"movieId, emb, bucketId schema:\")\n",
    "    embBucketResult.printSchema()\n",
    "    print(\"movieId, emb, bucketId data result:\")\n",
    "    embBucketResult.show(10, truncate=False)\n",
    "    print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\")\n",
    "    sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237)\n",
    "    bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)\n",
    "\n",
    "\n",
    "def trainItem2vec(spark, samples, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - \n",
    "    \"\"\"\n",
    "    word2vec = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "    model = word2vec.fit(samples)\n",
    "    synonyms = model.findSynonyms(\"158\", 20)\n",
    "    for synonym, cosineSimilarity in synonyms:\n",
    "        print(synonym, cosineSimilarity)\n",
    "    embOutputDir = '/'.join(embOutputPath.split('/')[:-1])\n",
    "    if not os.path.exists(embOutputDir):\n",
    "        os.makedirs(embOutputDir)\n",
    "    with open(embOutputPath, 'w') as f:\n",
    "        for movie_id in model.getVectors():\n",
    "            vectors = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "            f.write(movie_id + \":\" + vectors + \"\\n\")\n",
    "    embeddingLSH(spark, model.getVectors())\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_pair(x):\n",
    "    # eg:\n",
    "    # watch sequence:['858', '50', '593', '457']\n",
    "    # return:[['858', '50'],['50', '593'],['593', '457']]\n",
    "    pairSeq = []\n",
    "    previousItem = ''\n",
    "    for item in x:\n",
    "        if not previousItem:\n",
    "            previousItem = item\n",
    "        else:\n",
    "            pairSeq.append((previousItem, item))\n",
    "            previousItem = item\n",
    "    return pairSeq\n",
    "\n",
    "\n",
    "def generateTransitionMatrix(samples):\n",
    "    pairSamples = samples.flatMap(lambda x: generate_pair(x))\n",
    "    pairCountMap = pairSamples.countByValue()\n",
    "    pairTotalCount = 0\n",
    "    transitionCountMatrix = defaultdict(dict)\n",
    "    itemCountMap = defaultdict(int)\n",
    "    for key, cnt in pairCountMap.items():\n",
    "        key1, key2 = key\n",
    "        transitionCountMatrix[key1][key2] = cnt\n",
    "        itemCountMap[key1] += cnt\n",
    "        pairTotalCount += cnt\n",
    "    transitionMatrix = defaultdict(dict)\n",
    "    itemDistribution = defaultdict(dict)\n",
    "    for key1, transitionMap in transitionCountMatrix.items():\n",
    "        for key2, cnt in transitionMap.items():\n",
    "            transitionMatrix[key1][key2] = transitionCountMatrix[key1][key2] / itemCountMap[key1]\n",
    "    for itemid, cnt in itemCountMap.items():\n",
    "        itemDistribution[itemid] = cnt / pairTotalCount\n",
    "    return transitionMatrix, itemDistribution\n",
    "\n",
    "\n",
    "def oneRandomWalk(transitionMatrix, itemDistribution, sampleLength):\n",
    "    sample = []\n",
    "    # pick the first element\n",
    "    randomDouble = random.random()\n",
    "    firstItem = \"\"\n",
    "    accumulateProb = 0.0\n",
    "    for item, prob in itemDistribution.items():\n",
    "        accumulateProb += prob\n",
    "        if accumulateProb >= randomDouble:\n",
    "            firstItem = item\n",
    "            break\n",
    "    sample.append(firstItem)\n",
    "    curElement = firstItem\n",
    "    i = 1\n",
    "    while i < sampleLength:\n",
    "        if (curElement not in itemDistribution) or (curElement not in transitionMatrix):\n",
    "            break\n",
    "        probDistribution = transitionMatrix[curElement]\n",
    "        randomDouble = random.random()\n",
    "        accumulateProb = 0.0\n",
    "        for item, prob in probDistribution.items():\n",
    "            accumulateProb += prob\n",
    "            if accumulateProb >= randomDouble:\n",
    "                curElement = item\n",
    "                break\n",
    "        sample.append(curElement)\n",
    "        i += 1\n",
    "    return sample\n",
    "\n",
    "\n",
    "def randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength):\n",
    "    samples = []\n",
    "    for i in range(sampleCount):\n",
    "        samples.append(oneRandomWalk(transitionMatrix, itemDistribution, sampleLength))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def graphEmb(samples, spark, embLength, embOutputFilename, saveToRedis, redisKeyPrefix):\n",
    "    transitionMatrix, itemDistribution = generateTransitionMatrix(samples)\n",
    "    sampleCount = 20000\n",
    "    sampleLength = 10\n",
    "    newSamples = randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength)\n",
    "    rddSamples = spark.sparkContext.parallelize(newSamples)\n",
    "    trainItem2vec(spark, rddSamples, embLength, embOutputFilename, saveToRedis, redisKeyPrefix)\n",
    "\n",
    "\n",
    "def generateUserEmb(spark, rawSampleDataPath, model, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "    ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "    Vectors_list = []\n",
    "    for key, value in model.getVectors().items():\n",
    "        Vectors_list.append((key, list(value)))\n",
    "    fields = [\n",
    "        StructField('movieId', StringType(), False),\n",
    "        StructField('emb', ArrayType(FloatType()), False)\n",
    "    ]\n",
    "    schema = StructType(fields)\n",
    "    Vectors_df = spark.createDataFrame(Vectors_list, schema=schema)\n",
    "    ratingSamples = ratingSamples.join(Vectors_df, on='movieId', how='inner')\n",
    "    result = ratingSamples.select('userId', 'emb').rdd.map(lambda x: (x[0], x[1])) \\\n",
    "        .reduceByKey(lambda a, b: [a[i] + b[i] for i in range(len(a))]).collect()\n",
    "    with open(embOutputPath, 'w') as f:\n",
    "        for row in result:\n",
    "            vectors = \" \".join([str(emb) for emb in row[1]])\n",
    "            f.write(row[0] + \":\" + vectors + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ItemSquence...\n",
      "Train Item2Vec...\n",
      "48 0.9554703831672668\n",
      "256 0.9526916146278381\n",
      "186 0.9149225950241089\n",
      "31 0.9058868885040283\n",
      "168 0.8825162053108215\n",
      "355 0.8798472285270691\n",
      "277 0.8489635586738586\n",
      "252 0.846895158290863\n",
      "432 0.838388979434967\n",
      "552 0.8213717341423035\n",
      "520 0.8202667236328125\n",
      "276 0.8162290453910828\n",
      "44 0.8061022162437439\n",
      "236 0.799111008644104\n",
      "2 0.778323769569397\n",
      "455 0.776578426361084\n",
      "435 0.753156304359436\n",
      "60 0.7498870491981506\n",
      "204 0.7483898401260376\n",
      "169 0.747954249382019\n",
      "movieId, emb, bucketId schema:\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- emb: vector (nullable = true)\n",
      " |-- bucketId: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n",
      "movieId, emb, bucketId data result:\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|movieId|emb                                                                                                                    |bucketId                |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|710    |[1.0540144,-0.51502746,0.5281605,1.450357,0.057339992,0.4474986,2.2844045,0.3482987,-0.38938516,0.027673304]           |[[-2.0], [-3.0], [11.0]]|\n",
      "|205    |[0.1454958,-0.553919,-0.4468291,-0.27230924,-0.3761929,0.102063574,0.7027629,0.14570396,-0.6743555,-0.18847004]        |[[4.0], [-5.0], [3.0]]  |\n",
      "|45     |[-0.03892235,-0.23586328,-0.29967532,-0.7591808,-0.14348046,0.2656267,0.27896413,0.04461062,-0.7456205,-0.38145855]    |[[3.0], [-7.0], [-2.0]] |\n",
      "|515    |[-0.48615855,-0.053853642,-0.7212435,-0.66967344,-0.17020006,-0.086484775,0.21780819,0.12779585,-0.6983175,0.118902594]|[[7.0], [-1.0], [-1.0]] |\n",
      "|574    |[0.019824268,-0.8582355,0.0725144,-0.21314842,-0.13422251,0.51599413,0.48104072,0.15684187,-0.7856732,-0.27847546]     |[[-2.0], [-7.0], [0.0]] |\n",
      "|858    |[-0.5001281,-0.028307308,0.4028408,0.42118353,0.4303246,-0.23282114,0.03933488,0.017857382,-0.20825732,-0.10594034]    |[[-2.0], [2.0], [-1.0]] |\n",
      "|619    |[0.5319377,-0.41638327,0.21496852,0.94410706,0.032214817,0.32041118,1.4816011,0.19412938,-0.21489541,0.07552953]       |[[0.0], [-1.0], [7.0]]  |\n",
      "|507    |[0.16697133,-0.317201,-0.41770357,0.036486104,0.1727135,0.67130256,-0.04659112,-0.20308009,-0.35874993,-0.14167923]    |[[2.0], [-3.0], [-2.0]] |\n",
      "|113    |[0.21022719,-0.73415077,-0.29109964,0.9600509,0.26244825,0.22212721,1.6373941,0.14758147,-0.19319147,0.29030785]       |[[2.0], [1.0], [11.0]]  |\n",
      "|34     |[0.14202428,-0.7617257,0.071497105,-0.042020738,-0.0019254552,-0.5172049,-0.19705471,0.19891505,-0.11128272,0.10664987]|[[-5.0], [-4.0], [5.0]] |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Approximately searching for 5 nearest neighbors of the sample embedding:\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|movieId|emb                                                                                                                    |bucketId               |distCol           |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|673    |[0.6108411,0.42233306,0.37378806,0.3233045,0.37807748,-0.10918667,0.9630738,0.51603454,-0.11294678,-0.3275772]         |[[-1.0], [-4.0], [3.0]]|1.7941324038926134|\n",
      "|231    |[0.8726064,-0.6470386,0.22526874,0.15667517,0.5092644,-0.11466662,-0.40908486,0.053470764,0.15169394,-0.49648446]      |[[-9.0], [-8.0], [4.0]]|1.9834851061518963|\n",
      "|110    |[0.1367259,-0.020243065,-0.05236852,0.4448123,0.24456292,-0.4742553,-0.46431315,-0.24785504,-0.32048917,0.281163]      |[[-1.0], [0.0], [-1.0]]|2.033976383761452 |\n",
      "|661    |[0.12250397,0.24929881,0.13166462,0.0025487698,0.30972567,-0.2600373,0.83199334,0.37824103,-0.13570906,-0.6007044]     |[[1.0], [-3.0], [3.0]] |2.0534730857203645|\n",
      "|527    |[-0.30748498,-0.053641386,0.17465411,0.060966134,0.21488412,-0.61047155,-0.15355086,0.057479504,-0.30587265,0.20797937]|[[-2.0], [0.0], [0.0]] |2.0957001372854904|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "\n",
      "Train graph Embedding...\n",
      "186 0.9399803876876831\n",
      "31 0.9310539960861206\n",
      "256 0.9261811375617981\n",
      "168 0.9186466932296753\n",
      "368 0.9017253518104553\n",
      "204 0.8758782744407654\n",
      "252 0.8720588088035583\n",
      "172 0.8678579926490784\n",
      "48 0.863376796245575\n",
      "196 0.8535152673721313\n",
      "236 0.8499699234962463\n",
      "289 0.8407750725746155\n",
      "355 0.838380753993988\n",
      "552 0.8341473340988159\n",
      "151 0.8249722123146057\n",
      "44 0.8234415054321289\n",
      "224 0.8086671829223633\n",
      "370 0.8064671754837036\n",
      "276 0.7974141240119934\n",
      "520 0.7953224182128906\n",
      "movieId, emb, bucketId schema:\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- emb: vector (nullable = true)\n",
      " |-- bucketId: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n",
      "movieId, emb, bucketId data result:\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|movieId|emb                                                                                                                  |bucketId                |\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|710    |[1.2322347,0.16895808,0.42931515,0.15626639,0.3236185,0.07846603,0.53171283,0.54559886,0.58268005,-0.05246176]       |[[-7.0], [-6.0], [6.0]] |\n",
      "|205    |[0.6958414,0.40795514,-0.009168048,-0.3542839,-0.3779681,0.45482814,0.41975397,0.2709312,-0.20869693,0.06553435]     |[[1.0], [-5.0], [-2.0]] |\n",
      "|45     |[0.16972294,0.15502757,-0.3032932,-0.49640802,0.15000477,0.15923107,0.13917883,0.10865843,-0.18560398,0.5151765]     |[[1.0], [-3.0], [-1.0]] |\n",
      "|515    |[-0.1355237,0.14759001,-0.33924833,-0.5325125,-0.107972465,0.26822743,0.4139355,-0.28978664,-0.3249816,0.50561196]   |[[4.0], [-2.0], [-3.0]] |\n",
      "|574    |[1.0459907,-0.04783866,-0.23520479,-0.37929505,-0.20024936,0.099751346,-0.09655846,0.448661,-0.33609045,0.48249266]  |[[-1.0], [-7.0], [2.0]] |\n",
      "|858    |[-0.2740862,-0.14709686,0.13058881,0.4601685,-0.13488972,0.36766312,-0.07748305,0.45802745,0.04216897,0.14628342]    |[[-1.0], [5.0], [0.0]]  |\n",
      "|619    |[1.1484997,0.22476162,-0.038827047,-0.22485957,0.1489101,0.35001245,0.1740653,0.33387798,-0.18988138,0.07686789]     |[[-2.0], [-8.0], [0.0]] |\n",
      "|507    |[0.2916652,-0.09546944,0.27707118,-0.34634376,0.33794892,-0.059091955,-0.0902479,0.15171182,-0.20944837,0.37704355]  |[[-5.0], [-6.0], [-1.0]]|\n",
      "|113    |[1.1042031,0.6060762,0.14310445,0.02224311,0.30606657,7.456416E-6,0.11599636,0.21357717,0.15067935,0.2607651]        |[[-3.0], [-5.0], [0.0]] |\n",
      "|34     |[-0.16783164,0.36099303,0.15585437,-0.037834037,-0.5871588,-0.16798319,-0.02284482,0.1691373,-0.0108423475,0.2633466]|[[1.0], [3.0], [-2.0]]  |\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Approximately searching for 5 nearest neighbors of the sample embedding:\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|movieId|emb                                                                                                              |bucketId               |distCol           |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|836    |[1.0528573,0.33895427,0.6294386,0.13048036,0.43996274,0.028281,-0.1980697,0.46195424,0.48402253,-0.05034756]     |[[-9.0], [-5.0], [1.0]]|1.3569761290962867|\n",
      "|153    |[-0.20721631,0.50782675,0.95114654,0.3285679,0.40620616,-0.5975689,0.45902735,-0.19455203,0.14672351,-0.19017138]|[[-5.0], [0.0], [-2.0]]|1.4735753389806998|\n",
      "|414    |[0.26897326,0.694944,0.4227652,-0.082158364,0.29195687,-0.24629378,0.23407763,0.4281068,0.44660276,0.060402595]  |[[-3.0], [0.0], [1.0]] |1.5065597807810742|\n",
      "|748    |[0.9622548,0.062694885,0.5309618,0.09276987,0.42129502,0.18462588,-0.03265658,0.69441384,0.4942232,0.1301593]    |[[-9.0], [-5.0], [3.0]]|1.641377295026164 |\n",
      "|71     |[0.72963315,-0.10944478,0.2966183,-0.1419375,0.4283504,-0.22018869,0.13182826,0.07298865,0.15585506,-0.12607768] |[[-6.0], [-7.0], [3.0]]|1.7152461260165035|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "\n",
      "Train User Embedding...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    # Change to your own filepath\n",
    "    file_path = '../../data/'\n",
    "    rawSampleDataPath = file_path + \"ratings.csv\"\n",
    "    embLength = 10\n",
    "    print(\"Process ItemSquence...\")\n",
    "    samples = processItemSequence(spark, rawSampleDataPath)\n",
    "    print(\"Train Item2Vec...\")\n",
    "    model = trainItem2vec(spark, samples, embLength,\n",
    "                          embOutputPath=file_path + \"modeldata/item2vecEmb.csv\", saveToRedis=False,\n",
    "                          redisKeyPrefix=\"i2vEmb\")\n",
    "    print(\"Train graph Embedding...\")\n",
    "    graphEmb(samples, spark, embLength, embOutputFilename=file_path + \"modeldata/itemGraphEmb.csv\",\n",
    "             saveToRedis=True, redisKeyPrefix=\"graphEmb\")\n",
    "    print(\"Train User Embedding...\")\n",
    "    generateUserEmb(spark, rawSampleDataPath, model, embLength,\n",
    "                    embOutputPath=file_path + \"modeldata/userEmb.csv\", saveToRedis=False,\n",
    "                    redisKeyPrefix=\"uEmb\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "+ Difference between mllib and ml in pyspark\n",
    "    - mllib provides the RDD-based functions that take RDD pipeline class instance as input\n",
    "    - ml provides dataframe-based functions that take Spark Dataframe as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
