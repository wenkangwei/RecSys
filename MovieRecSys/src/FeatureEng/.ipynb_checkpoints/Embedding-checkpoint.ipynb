{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepWalk for Graph Embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "# from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.context import SparkContext as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 1760) (node1228.palmetto.clemson.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1099.00000000600092B0.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:819)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$1290.0000000088104DE0.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$1288.0000000088103F90.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDD$$Lambda$795.0000000010829C80.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1099.00000000600092B0.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-fdd8e6d8527b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrdd_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# type(x[0][1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \"\"\"\n\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/mlenv/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 107.0 failed 1 times, most recent failure: Lost task 0.0 in stage 107.0 (TID 1760) (node1228.palmetto.clemson.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1099.00000000600092B0.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:819)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$1290.0000000088104DE0.apply(Unknown Source)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$1288.0000000088103F90.apply(Unknown Source)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDD$$Lambda$795.0000000010829C80.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor142.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:819)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 594, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2916, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 418, in func\n    return f(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/rdd.py\", line 2144, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 240, in mergeValues\n    for k, v in iterator:\n  File \"/home/wenkanw/.conda/envs/mlenv/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$1099.00000000600092B0.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# conf = SparkConf().setAppName(\"app-name-of-your-choice\").setMaster(\"local[*]\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "# rdd_data= spark.sparkContext.parallelize([(1,2), (1,2), (2,1), (3,1), (2,1)], 2)#.countByValue().keys()\n",
    "rdd_data= sc.parallelize([(1,2), (1,2), (2,1), (3,1), (2,1)], 2)#.countByValue().keys()\n",
    "# sorted(rdd_data.flatMap(lambda x: range(1, x)).collect())\n",
    "# print(rdd_data.flatMap(lambda x: (1, x+1)).collect())\n",
    "# sorted(rdd_data.flatMap(lambda x: [(len(x), x)]).collect())\n",
    "from operator import add\n",
    "import numpy as np\n",
    "x = rdd_data.map(lambda x,y: (str(x),y, np.array(x),y)).reduceByKey(add).collect()\n",
    "type(x[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+\n",
      "|  a|  b|                   c|\n",
      "+---+---+--------------------+\n",
      "|  1|3.1|[1.0, 2.0, 3.0, 4.0]|\n",
      "|  2|4.1|[6.0, 7.0, 8.0, 4.0]|\n",
      "|  3|5.1|[8.0, 9.0, 10.0, ...|\n",
      "|  3|5.1|[8.0, 9.0, 10.0, ...|\n",
      "|  2|5.1|[8.0, 9.0, 10.0, ...|\n",
      "|  3|5.1|[8.0, 9.0, 10.0, ...|\n",
      "+---+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fields = [StructField('a', IntegerType(), False),\n",
    "StructField('b', FloatType(), False),\n",
    "StructField('c', ArrayType(FloatType()), False)]\n",
    "\n",
    "schema = StructType(fields)\n",
    "data = [(1, 3.1, [1.,2.,3.,4.]),\n",
    "       (2, 4.1, [6.,7.,8.,4.]),\n",
    "       (3, 5.1, [8.,9.,10.,4.]),\n",
    "       (3, 5.1, [8.,9.,10.,4.]),\n",
    "       (2, 5.1, [8.,9.,10.,4.]),\n",
    "       (3, 5.1, [8.,9.,10.,4.]),]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, array([1., 2., 3., 4.])),\n",
       " (2, [14.0, 16.0, 18.0, 8.0]),\n",
       " (3, [24.0, 27.0, 30.0, 12.0])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import sql\n",
    "df.select('a','b','c').withColumn(\"cnt\", F.count(\"a\").over(sql.Window.partitionBy('a')))\\\n",
    ".rdd.map(lambda a: (a[0],np.array(a[2]))).reduceByKey(lambda a,b: (a+b).tolist()).collect()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ItemSquence...\n",
      "258 0.9690099358558655\n",
      "146 0.944780707359314\n",
      "498 0.9190452694892883\n",
      "528 0.9117680788040161\n",
      "340 0.9110337495803833\n",
      "502 0.9103050231933594\n",
      "325 0.9093467593193054\n",
      "606 0.9086456298828125\n",
      "575 0.8975323438644409\n",
      "577 0.8879609107971191\n",
      "437 0.8824750781059265\n",
      "382 0.8805188536643982\n",
      "89 0.87254399061203\n",
      "271 0.8707731366157532\n",
      "378 0.8699591755867004\n",
      "291 0.8654010891914368\n",
      "177 0.8641008138656616\n",
      "455 0.8567240834236145\n",
      "343 0.8538267016410828\n",
      "42 0.8516796827316284\n",
      "\n",
      "User Embdding\n",
      "+-------+------+------+----------+--------------------+\n",
      "|movieId|userId|rating| timestamp|                 emb|\n",
      "+-------+------+------+----------+--------------------+\n",
      "|    296|     1|   4.0|1112484767|[-0.1762909, 0.05...|\n",
      "|    296|     8|   5.0| 833973081|[-0.1762909, 0.05...|\n",
      "|    296|    11|   3.5|1230858799|[-0.1762909, 0.05...|\n",
      "|    296|    13|   5.0| 849082366|[-0.1762909, 0.05...|\n",
      "|    296|    15|   3.0| 840206642|[-0.1762909, 0.05...|\n",
      "+-------+------+------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- emb: array (nullable = false)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n",
      "[('248', [-0.7311561740934849, 1.9928374690935016, 2.592155239544809, 6.189928602427244, 2.156123681459576, 3.184986088424921, 1.2312789936549962, 0.7421017624437809, 1.5366021879017353, 5.004217987880111]), ('966', [-2.887468671426177, 8.727703409269452, -0.004500298760831356, 3.099868013843661, -1.5243244851008058, -6.703906386625022, 9.88023172179237, 2.8009974909946322, 2.582734556403011, 2.0053618904203176]), ('1013', [-3.250557791441679, 2.148715676856227, -0.09281204640865326, 3.9754957370460033, 3.074593438068405, -2.0245465636253357, 3.7061097626574337, -0.8732031788676977, 1.351111026480794, 1.6717791464179754]), ('1114', [10.067272461019456, 34.64158163848333, 0.7612712609334267, 3.6519750202132855, 17.450458033708856, -1.8658973505953327, 30.163224864401855, -3.4456612188369036, -0.746874175267294, -18.559408934786916]), ('1729', [-17.667664843094826, 33.38383792876266, -7.860437464318238, 47.01111673688865, 17.768879835493863, 39.70112303458154, 17.55344808773225, -6.515104988124222, -15.083830921299523, -9.604371157940477])]\n",
      "User Embedding Saved!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def getItemSeqs(spark, samplesRating):\n",
    "    \"\"\"\n",
    "    extract item sequences for each user from dataframe\n",
    "    1. for each user, collect the corresponding visited movies and timestamp into a list\n",
    "    2. use UDF to process movie list and timestamp list to sort the movie sequence for each user\n",
    "    3. join the movie list to get a string for each user\n",
    "    \"\"\"\n",
    "    def sortF(movie_list, timestamp_list):\n",
    "        \"\"\"\n",
    "        sort by time and return the corresponding movie sequence\n",
    "        eg:\n",
    "            input: movie_list:[1,2,3]\n",
    "                   timestamp_list:[1112486027,1212546032,1012486033]\n",
    "            return [3,1,2]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        # concat timestamp with movie id\n",
    "        for m, t in zip(movie_list, timestamp_list):\n",
    "            pairs.append((m, t))\n",
    "        # sort by time\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])\n",
    "        return [x[0] for x in pairs]\n",
    "    \n",
    "    \n",
    "    sortUDF = udf(sortF, ArrayType(StringType()))\n",
    "    \n",
    "    # rating data\n",
    "    #ratingSamples.show(5)\n",
    "    # ratingSamples.printSchema()\n",
    "    userSequence = samplesRating.where(F.col(\"rating\") > 3) \\\n",
    "                    .groupBy(\"userId\")\\\n",
    "                    .agg(sortUDF(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias(\"movieIds\"))\\\n",
    "                    .withColumn(\"movieIdStr\", F.array_join(F.col(\"movieIds\"), \" \"))\n",
    "    seq = userSequence.select(\"movieIdStr\").rdd.map(lambda x : x[0].split(\" \"))\n",
    "    #print(seq.collect()[:5])\n",
    "    return seq\n",
    "\n",
    "\n",
    "\n",
    "def embeddingLSH(spark, movieEmbMap):\n",
    "    \"\"\"\n",
    "    Local sensitive hashing using bucketedRandomProjection\n",
    "    \"\"\"\n",
    "    movieEmbSeq = []\n",
    "    for key, embedding_list in movieEmbMap.items():\n",
    "        embedding_list = [np.float64(embedding) for embedding in embedding_list]\n",
    "        movieEmbSeq.append((key, Vectors.dense(embedding_list)))\n",
    "    movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\")\n",
    "    bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1,\n",
    "                                                      numHashTables=3)\n",
    "    bucketModel = bucketProjectionLSH.fit(movieEmbDF)\n",
    "    embBucketResult = bucketModel.transform(movieEmbDF)\n",
    "    print(\"movieId, emb, bucketId schema:\")\n",
    "    embBucketResult.printSchema()\n",
    "    print(\"movieId, emb, bucketId data result:\")\n",
    "    embBucketResult.show(10, truncate=False)\n",
    "    print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\")\n",
    "    sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237)\n",
    "    bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "def getTransitionMatrix(item_seq):\n",
    "    \"\"\"\n",
    "    build graph and transition matrix based on input item sequences \n",
    "    input: list of item sequence in RDD format\n",
    "    output: transition matrix and item distribution in dictionary format\n",
    "    \n",
    "    \"\"\"\n",
    "    def generate_pair(x):\n",
    "        \"\"\"\n",
    "        use a sliding window with size of 2 to generate item pairs\n",
    "        input: ls =  list of items \n",
    "        output: list of pairs\n",
    "        example:\n",
    "            input: [86, 90, 11, 100,]\n",
    "            output: [[86,90], [90, 11], [11,100]]\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        prev = None\n",
    "        print(x)\n",
    "        for i in range(len(x)):\n",
    "            if i >0:\n",
    "                res.append((x[i-1],x[i]))\n",
    "        return res\n",
    "\n",
    "    \n",
    "    #  convert item sequences to pair list \n",
    "    pair_seq = item_seq.flatMap(lambda x: generate_pair(x))\n",
    "    # convert pair list to  dictionary, key = pair, value = count\n",
    "    pair_count_dict = pair_seq.countByValue()\n",
    "    tot_count = pair_seq.count()\n",
    "    trans_matrix =  defaultdict(dict)\n",
    "    item_count = defaultdict(int)\n",
    "    item_dist = defaultdict(float)\n",
    "    \n",
    "    # consider out-degree only \n",
    "    for item, cnt in pair_count_dict.items():\n",
    "        item1, item2 = item[0], item[1]\n",
    "        item_count[item1] +=  cnt\n",
    "        trans_matrix[item1][item2] = cnt\n",
    "        \n",
    "    for item, cnt in pair_count_dict.items():\n",
    "        item1, item2 = item[0], item[1]\n",
    "        # possibility of transition\n",
    "        trans_matrix[item1][item2] /= item_count[item1]\n",
    "        # distribution of each source node (item)\n",
    "        item_dist[item1] =  item_count[item1]/tot_count\n",
    "        \n",
    "    return trans_matrix, item_dist\n",
    "\n",
    "def oneRandomWalk(trans_mat, item_dist, sample_length):\n",
    "    \"\"\"\n",
    "    generate one random walk sequence based on transition matrix\n",
    "    input: \n",
    "        - trans_mat: transition matrix\n",
    "        - item_dist: distribution of item\n",
    "        - sample length: number of node in a path  = length of a walk -1 = length of edges - 1\n",
    "    \"\"\"\n",
    "    rand_val = random.random()\n",
    "    # randomly pick item based on CDF , cumulative density function, obtained from the item distribution\n",
    "    # we can also randomly pick a item based on the distribution using  choice () function from numpy as well\n",
    "    cdf_prob =0\n",
    "    first_item = ''\n",
    "    for item, prob in item_dist.items():\n",
    "        cdf_prob += prob\n",
    "        if cdf_prob >= rand_val:\n",
    "            first_item = item\n",
    "            break\n",
    "    item_list = [first_item]\n",
    "    cur_item = first_item\n",
    "    \n",
    "    while len(item_list) < sample_length:\n",
    "        if (cur_item not in item_dist) or (cur_item not in trans_mat):\n",
    "            break\n",
    "        cdf_prob = 0\n",
    "        rand_val = random.random()\n",
    "        dist = trans_mat[cur_item]\n",
    "        for item, prob in dist.items():\n",
    "            cdf_prob += prob\n",
    "            if cdf_prob >= rand_val:\n",
    "                cur_item = item\n",
    "                break\n",
    "        item_list.append(cur_item)\n",
    "        \n",
    "    return item_list\n",
    "\n",
    "\n",
    "\n",
    "def generateItemSeqs(trans_mat, item_dist, num_seq=20000, sample_length = 10  ):\n",
    "    \"\"\"\n",
    "    use random walk to generate multiple item sequences\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for i in range(num_seq):\n",
    "        samples.append(oneRandomWalk(trans_mat, item_dist, sample_length))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def trainItem2Vec(spark, item_seqs, emb_length, output_path, save_to_redis=False, redis_keyprefix=None):\n",
    "    \"\"\"\n",
    "    use Word2Vec to train item embedding\n",
    "    input:\n",
    "        - item_seqs: RDD pipeline instance, rather than dataframe\n",
    "    Note:  \n",
    "    - Word2Vec from mllib is a function that take RDD pipeline as input.\n",
    "    - Word2Vec from ml is a function that take Dataframe as input \n",
    "    \n",
    "    \"\"\"\n",
    "    # train word2Vec\n",
    "#     w2v = Word2Vec(vectorSize=emb_length, windowSize = 5, maxIter = 10, seed=42)\n",
    "    w2v = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "    model = w2v.fit(item_seqs)\n",
    "    # test word2vec\n",
    "    synonyms = model.findSynonyms(\"157\", 20)\n",
    "    for synonym, cos_similarity in synonyms:\n",
    "        print(synonym, cos_similarity)\n",
    "    \n",
    "    # save word2Vec to input path \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        for movie_id in model.getVectors():\n",
    "            # convert vector to string type and store it\n",
    "            vector = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "            pair = movie_id + \":\" + vector + \"\\n\"\n",
    "            fp.write(pair)\n",
    "    return model\n",
    "\n",
    "\n",
    "def getDeepWalk(spark, item_seq, sample_length=10, num_walk=20000, output_file='../../data/modeldata/embedding.csv',\n",
    "             save_to_redis=False, redis_key_prefix=None):\n",
    "    \"\"\"\n",
    "    use DeepWalk to generate graph embeddings\n",
    "    input:\n",
    "        - item_seq: RDD based sequence of item visited by a user\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # construct probability graph\n",
    "    trans_mat, item_dist = getTransitionMatrix(item_seq)\n",
    "    \n",
    "    # generate sequence samples randomly\n",
    "    samples = generateItemSeqs(trans_mat, item_dist,num_seq=num_walk, sample_length = sample_length )\n",
    "    # convert list of samples to spark rdd \n",
    "    samples_rdd = spark.sparkContext.parallelize(samples)\n",
    "    # train item2Vec\n",
    "    graphEmbModel = trainItem2Vec(spark, samples_rdd, emb_length=10, output_path=output_file , save_to_redis=False, redis_keyprefix=None)\n",
    "    \n",
    "    return graphEmbModel\n",
    "\n",
    "def getUserEmb( spark ,samples_rating, item_emb_model, output_file):\n",
    "    \"\"\"\n",
    "    generate user embedding based on item embedding\n",
    "    use map reduce to sum up embeddings of items purchased by user to generate user embedding\n",
    "    input:\n",
    "        - spark: spark session\n",
    "        - samples_rating: dataframe with rating, movieId, userId data\n",
    "        - item_emb_model: word2Vec/Item2Vec model trained by deep walk. \n",
    "        - output_file: file name of user embedding \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "#     assert not item_emb or not item_emb_path, \"Must input either item embedding vectors or path\"\n",
    "#     if item_emb_path != None:\n",
    "#         item_emb = spark.read.csv(item_emb_path, header=True)\n",
    "\n",
    "    emb_dict = item_emb_model.getVectors()\n",
    "    item_emb_ls=[]\n",
    "    for item, emb in emb_dict.items():\n",
    "        #print((item, emb))\n",
    "        item_emb_ls.append((item, list(emb)))\n",
    "    fields = [StructField('movieId', StringType(),False),\n",
    "             StructField('emb', ArrayType(FloatType()),False),]\n",
    "    item_emb_schema = StructType(fields)\n",
    "    item_emb_df = spark.createDataFrame(item_emb_ls, item_emb_schema)\n",
    "    \n",
    "    # apply mapreduce to sum up item embeddings for each user to obtain user embedding\n",
    "    # Note: we need inner join here to avoid empty item embedding during mapreduce calculation\n",
    "    user_emb = samples_rating.join(item_emb_df, on=\"movieId\", how=\"inner\")\n",
    "    print()\n",
    "    print(\"User Embdding\")\n",
    "    user_emb.show(5)\n",
    "    user_emb.printSchema()\n",
    "    user_emb = user_emb.select(\"userId\",\"emb\").rdd.map(lambda row: (row[0], row[1]) ).reduceByKey(lambda emb1, emb2: [ float(emb1[i]) + float(emb2[i]) for i in range(len(emb1))] ).collect()\n",
    "    print(user_emb[:5])\n",
    "    #save user embedding\n",
    "    with open(output_file,\"w\") as fp:\n",
    "        for userId, emb in user_emb:\n",
    "            row = \" \".join([str(e) for e in emb])\n",
    "            row = str(userId)+ \":\"+ row + \"\\n\"\n",
    "            fp.write(row)\n",
    "    print(\"User Embedding Saved!\")\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    # Change to your own filepath\n",
    "    file_path = '../../data/'\n",
    "    rawSampleDataPath = file_path + \"ratings.csv\"\n",
    "    embLength = 10\n",
    "    print(\"Process ItemSquence...\")\n",
    "    samplesRating = spark.read.csv(rawSampleDataPath, header = True)\n",
    "    item_seqs = getItemSeqs(spark, samplesRating)\n",
    "    #print(samples)\n",
    "    \n",
    "#     trainItem2Vec(item_seqs, emb_length=10, output_path=file_path+\"modeldata/itemGraphEmb.csv\", save_to_redis=False, redis_keyprefix=None)\n",
    "    \n",
    "    graphEmb = getDeepWalk(spark, item_seqs, sample_length=10, num_walk=20000, output_file=file_path+\"modeldata/itemGraphEmb.csv\",\n",
    "             save_to_redis=False, redis_key_prefix=None)\n",
    "    getUserEmb( spark ,samples_rating= samplesRating, item_emb_model= graphEmb, output_file= file_path+\"modeldata/userEmb.csv\")\n",
    "\n",
    "    print(\"Done!\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in graphEmb.findSynonyms('1',10):\n",
    "#     print(i)\n",
    "\n",
    "# graphEmb.getVectors()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UdfFunction:\n",
    "    @staticmethod\n",
    "    def sortF(movie_list, timestamp_list):\n",
    "        \"\"\"\n",
    "        sort by time and return the corresponding movie sequence\n",
    "        eg:\n",
    "            input: movie_list:[1,2,3]\n",
    "                   timestamp_list:[1112486027,1212546032,1012486033]\n",
    "            return [3,1,2]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for m, t in zip(movie_list, timestamp_list):\n",
    "            pairs.append((m, t))\n",
    "        # sort by time\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])\n",
    "        return [x[0] for x in pairs]\n",
    "\n",
    "\n",
    "def processItemSequence(spark, rawSampleDataPath):\n",
    "    # rating data\n",
    "    ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "    # ratingSamples.show(5)\n",
    "    # ratingSamples.printSchema()\n",
    "    sortUdf = udf(UdfFunction.sortF, ArrayType(StringType()))\n",
    "    userSeq = ratingSamples \\\n",
    "        .where(F.col(\"rating\") >= 3.5) \\\n",
    "        .groupBy(\"userId\") \\\n",
    "        .agg(sortUdf(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias('movieIds')) \\\n",
    "        .withColumn(\"movieIdStr\", array_join(F.col(\"movieIds\"), \" \"))\n",
    "    # userSeq.select(\"userId\", \"movieIdStr\").show(10, truncate = False)\n",
    "    return userSeq.select('movieIdStr').rdd.map(lambda x: x[0].split(' '))\n",
    "\n",
    "\n",
    "def embeddingLSH(spark, movieEmbMap):\n",
    "    movieEmbSeq = []\n",
    "    for key, embedding_list in movieEmbMap.items():\n",
    "        embedding_list = [np.float64(embedding) for embedding in embedding_list]\n",
    "        movieEmbSeq.append((key, Vectors.dense(embedding_list)))\n",
    "    movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\")\n",
    "    bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1,\n",
    "                                                      numHashTables=3)\n",
    "    bucketModel = bucketProjectionLSH.fit(movieEmbDF)\n",
    "    embBucketResult = bucketModel.transform(movieEmbDF)\n",
    "    print(\"movieId, emb, bucketId schema:\")\n",
    "    embBucketResult.printSchema()\n",
    "    print(\"movieId, emb, bucketId data result:\")\n",
    "    embBucketResult.show(10, truncate=False)\n",
    "    print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\")\n",
    "    sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237)\n",
    "    bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)\n",
    "\n",
    "\n",
    "def trainItem2vec(spark, samples, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - \n",
    "    \"\"\"\n",
    "    word2vec = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "    model = word2vec.fit(samples)\n",
    "    synonyms = model.findSynonyms(\"158\", 20)\n",
    "    for synonym, cosineSimilarity in synonyms:\n",
    "        print(synonym, cosineSimilarity)\n",
    "    embOutputDir = '/'.join(embOutputPath.split('/')[:-1])\n",
    "    if not os.path.exists(embOutputDir):\n",
    "        os.makedirs(embOutputDir)\n",
    "    with open(embOutputPath, 'w') as f:\n",
    "        for movie_id in model.getVectors():\n",
    "            vectors = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "            f.write(movie_id + \":\" + vectors + \"\\n\")\n",
    "    embeddingLSH(spark, model.getVectors())\n",
    "    return model\n",
    "\n",
    "\n",
    "def generate_pair(x):\n",
    "    # eg:\n",
    "    # watch sequence:['858', '50', '593', '457']\n",
    "    # return:[['858', '50'],['50', '593'],['593', '457']]\n",
    "    pairSeq = []\n",
    "    previousItem = ''\n",
    "    for item in x:\n",
    "        if not previousItem:\n",
    "            previousItem = item\n",
    "        else:\n",
    "            pairSeq.append((previousItem, item))\n",
    "            previousItem = item\n",
    "    return pairSeq\n",
    "\n",
    "\n",
    "def generateTransitionMatrix(samples):\n",
    "    pairSamples = samples.flatMap(lambda x: generate_pair(x))\n",
    "    pairCountMap = pairSamples.countByValue()\n",
    "    pairTotalCount = 0\n",
    "    transitionCountMatrix = defaultdict(dict)\n",
    "    itemCountMap = defaultdict(int)\n",
    "    for key, cnt in pairCountMap.items():\n",
    "        key1, key2 = key\n",
    "        transitionCountMatrix[key1][key2] = cnt\n",
    "        itemCountMap[key1] += cnt\n",
    "        pairTotalCount += cnt\n",
    "    transitionMatrix = defaultdict(dict)\n",
    "    itemDistribution = defaultdict(dict)\n",
    "    for key1, transitionMap in transitionCountMatrix.items():\n",
    "        for key2, cnt in transitionMap.items():\n",
    "            transitionMatrix[key1][key2] = transitionCountMatrix[key1][key2] / itemCountMap[key1]\n",
    "    for itemid, cnt in itemCountMap.items():\n",
    "        itemDistribution[itemid] = cnt / pairTotalCount\n",
    "    return transitionMatrix, itemDistribution\n",
    "\n",
    "\n",
    "def oneRandomWalk(transitionMatrix, itemDistribution, sampleLength):\n",
    "    sample = []\n",
    "    # pick the first element\n",
    "    randomDouble = random.random()\n",
    "    firstItem = \"\"\n",
    "    accumulateProb = 0.0\n",
    "    for item, prob in itemDistribution.items():\n",
    "        accumulateProb += prob\n",
    "        if accumulateProb >= randomDouble:\n",
    "            firstItem = item\n",
    "            break\n",
    "    sample.append(firstItem)\n",
    "    curElement = firstItem\n",
    "    i = 1\n",
    "    while i < sampleLength:\n",
    "        if (curElement not in itemDistribution) or (curElement not in transitionMatrix):\n",
    "            break\n",
    "        probDistribution = transitionMatrix[curElement]\n",
    "        randomDouble = random.random()\n",
    "        accumulateProb = 0.0\n",
    "        for item, prob in probDistribution.items():\n",
    "            accumulateProb += prob\n",
    "            if accumulateProb >= randomDouble:\n",
    "                curElement = item\n",
    "                break\n",
    "        sample.append(curElement)\n",
    "        i += 1\n",
    "    return sample\n",
    "\n",
    "\n",
    "def randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength):\n",
    "    samples = []\n",
    "    for i in range(sampleCount):\n",
    "        samples.append(oneRandomWalk(transitionMatrix, itemDistribution, sampleLength))\n",
    "    return samples\n",
    "\n",
    "\n",
    "def graphEmb(samples, spark, embLength, embOutputFilename, saveToRedis, redisKeyPrefix):\n",
    "    transitionMatrix, itemDistribution = generateTransitionMatrix(samples)\n",
    "    sampleCount = 20000\n",
    "    sampleLength = 10\n",
    "    newSamples = randomWalk(transitionMatrix, itemDistribution, sampleCount, sampleLength)\n",
    "    rddSamples = spark.sparkContext.parallelize(newSamples)\n",
    "    trainItem2vec(spark, rddSamples, embLength, embOutputFilename, saveToRedis, redisKeyPrefix)\n",
    "\n",
    "\n",
    "def generateUserEmb(spark, rawSampleDataPath, model, embLength, embOutputPath, saveToRedis, redisKeyPrefix):\n",
    "    ratingSamples = spark.read.format(\"csv\").option(\"header\", \"true\").load(rawSampleDataPath)\n",
    "    Vectors_list = []\n",
    "    for key, value in model.getVectors().items():\n",
    "        Vectors_list.append((key, list(value)))\n",
    "    fields = [\n",
    "        StructField('movieId', StringType(), False),\n",
    "        StructField('emb', ArrayType(FloatType()), False)\n",
    "    ]\n",
    "    schema = StructType(fields)\n",
    "    Vectors_df = spark.createDataFrame(Vectors_list, schema=schema)\n",
    "    ratingSamples = ratingSamples.join(Vectors_df, on='movieId', how='inner')\n",
    "    result = ratingSamples.select('userId', 'emb').rdd.map(lambda x: (x[0], x[1])) \\\n",
    "        .reduceByKey(lambda a, b: [a[i] + b[i] for i in range(len(a))]).collect()\n",
    "    with open(embOutputPath, 'w') as f:\n",
    "        for row in result:\n",
    "            vectors = \" \".join([str(emb) for emb in row[1]])\n",
    "            f.write(row[0] + \":\" + vectors + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ItemSquence...\n",
      "Train Item2Vec...\n",
      "48 0.9554703831672668\n",
      "256 0.9526916146278381\n",
      "186 0.9149225950241089\n",
      "31 0.9058868885040283\n",
      "168 0.8825162053108215\n",
      "355 0.8798472285270691\n",
      "277 0.8489635586738586\n",
      "252 0.846895158290863\n",
      "432 0.838388979434967\n",
      "552 0.8213717341423035\n",
      "520 0.8202667236328125\n",
      "276 0.8162290453910828\n",
      "44 0.8061022162437439\n",
      "236 0.799111008644104\n",
      "2 0.778323769569397\n",
      "455 0.776578426361084\n",
      "435 0.753156304359436\n",
      "60 0.7498870491981506\n",
      "204 0.7483898401260376\n",
      "169 0.747954249382019\n",
      "movieId, emb, bucketId schema:\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- emb: vector (nullable = true)\n",
      " |-- bucketId: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n",
      "movieId, emb, bucketId data result:\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|movieId|emb                                                                                                                    |bucketId                |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|710    |[1.0540144,-0.51502746,0.5281605,1.450357,0.057339992,0.4474986,2.2844045,0.3482987,-0.38938516,0.027673304]           |[[-2.0], [-3.0], [11.0]]|\n",
      "|205    |[0.1454958,-0.553919,-0.4468291,-0.27230924,-0.3761929,0.102063574,0.7027629,0.14570396,-0.6743555,-0.18847004]        |[[4.0], [-5.0], [3.0]]  |\n",
      "|45     |[-0.03892235,-0.23586328,-0.29967532,-0.7591808,-0.14348046,0.2656267,0.27896413,0.04461062,-0.7456205,-0.38145855]    |[[3.0], [-7.0], [-2.0]] |\n",
      "|515    |[-0.48615855,-0.053853642,-0.7212435,-0.66967344,-0.17020006,-0.086484775,0.21780819,0.12779585,-0.6983175,0.118902594]|[[7.0], [-1.0], [-1.0]] |\n",
      "|574    |[0.019824268,-0.8582355,0.0725144,-0.21314842,-0.13422251,0.51599413,0.48104072,0.15684187,-0.7856732,-0.27847546]     |[[-2.0], [-7.0], [0.0]] |\n",
      "|858    |[-0.5001281,-0.028307308,0.4028408,0.42118353,0.4303246,-0.23282114,0.03933488,0.017857382,-0.20825732,-0.10594034]    |[[-2.0], [2.0], [-1.0]] |\n",
      "|619    |[0.5319377,-0.41638327,0.21496852,0.94410706,0.032214817,0.32041118,1.4816011,0.19412938,-0.21489541,0.07552953]       |[[0.0], [-1.0], [7.0]]  |\n",
      "|507    |[0.16697133,-0.317201,-0.41770357,0.036486104,0.1727135,0.67130256,-0.04659112,-0.20308009,-0.35874993,-0.14167923]    |[[2.0], [-3.0], [-2.0]] |\n",
      "|113    |[0.21022719,-0.73415077,-0.29109964,0.9600509,0.26244825,0.22212721,1.6373941,0.14758147,-0.19319147,0.29030785]       |[[2.0], [1.0], [11.0]]  |\n",
      "|34     |[0.14202428,-0.7617257,0.071497105,-0.042020738,-0.0019254552,-0.5172049,-0.19705471,0.19891505,-0.11128272,0.10664987]|[[-5.0], [-4.0], [5.0]] |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Approximately searching for 5 nearest neighbors of the sample embedding:\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|movieId|emb                                                                                                                    |bucketId               |distCol           |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|673    |[0.6108411,0.42233306,0.37378806,0.3233045,0.37807748,-0.10918667,0.9630738,0.51603454,-0.11294678,-0.3275772]         |[[-1.0], [-4.0], [3.0]]|1.7941324038926134|\n",
      "|231    |[0.8726064,-0.6470386,0.22526874,0.15667517,0.5092644,-0.11466662,-0.40908486,0.053470764,0.15169394,-0.49648446]      |[[-9.0], [-8.0], [4.0]]|1.9834851061518963|\n",
      "|110    |[0.1367259,-0.020243065,-0.05236852,0.4448123,0.24456292,-0.4742553,-0.46431315,-0.24785504,-0.32048917,0.281163]      |[[-1.0], [0.0], [-1.0]]|2.033976383761452 |\n",
      "|661    |[0.12250397,0.24929881,0.13166462,0.0025487698,0.30972567,-0.2600373,0.83199334,0.37824103,-0.13570906,-0.6007044]     |[[1.0], [-3.0], [3.0]] |2.0534730857203645|\n",
      "|527    |[-0.30748498,-0.053641386,0.17465411,0.060966134,0.21488412,-0.61047155,-0.15355086,0.057479504,-0.30587265,0.20797937]|[[-2.0], [0.0], [0.0]] |2.0957001372854904|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "\n",
      "Train graph Embedding...\n",
      "186 0.9399803876876831\n",
      "31 0.9310539960861206\n",
      "256 0.9261811375617981\n",
      "168 0.9186466932296753\n",
      "368 0.9017253518104553\n",
      "204 0.8758782744407654\n",
      "252 0.8720588088035583\n",
      "172 0.8678579926490784\n",
      "48 0.863376796245575\n",
      "196 0.8535152673721313\n",
      "236 0.8499699234962463\n",
      "289 0.8407750725746155\n",
      "355 0.838380753993988\n",
      "552 0.8341473340988159\n",
      "151 0.8249722123146057\n",
      "44 0.8234415054321289\n",
      "224 0.8086671829223633\n",
      "370 0.8064671754837036\n",
      "276 0.7974141240119934\n",
      "520 0.7953224182128906\n",
      "movieId, emb, bucketId schema:\n",
      "root\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- emb: vector (nullable = true)\n",
      " |-- bucketId: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      "\n",
      "movieId, emb, bucketId data result:\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|movieId|emb                                                                                                                  |bucketId                |\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "|710    |[1.2322347,0.16895808,0.42931515,0.15626639,0.3236185,0.07846603,0.53171283,0.54559886,0.58268005,-0.05246176]       |[[-7.0], [-6.0], [6.0]] |\n",
      "|205    |[0.6958414,0.40795514,-0.009168048,-0.3542839,-0.3779681,0.45482814,0.41975397,0.2709312,-0.20869693,0.06553435]     |[[1.0], [-5.0], [-2.0]] |\n",
      "|45     |[0.16972294,0.15502757,-0.3032932,-0.49640802,0.15000477,0.15923107,0.13917883,0.10865843,-0.18560398,0.5151765]     |[[1.0], [-3.0], [-1.0]] |\n",
      "|515    |[-0.1355237,0.14759001,-0.33924833,-0.5325125,-0.107972465,0.26822743,0.4139355,-0.28978664,-0.3249816,0.50561196]   |[[4.0], [-2.0], [-3.0]] |\n",
      "|574    |[1.0459907,-0.04783866,-0.23520479,-0.37929505,-0.20024936,0.099751346,-0.09655846,0.448661,-0.33609045,0.48249266]  |[[-1.0], [-7.0], [2.0]] |\n",
      "|858    |[-0.2740862,-0.14709686,0.13058881,0.4601685,-0.13488972,0.36766312,-0.07748305,0.45802745,0.04216897,0.14628342]    |[[-1.0], [5.0], [0.0]]  |\n",
      "|619    |[1.1484997,0.22476162,-0.038827047,-0.22485957,0.1489101,0.35001245,0.1740653,0.33387798,-0.18988138,0.07686789]     |[[-2.0], [-8.0], [0.0]] |\n",
      "|507    |[0.2916652,-0.09546944,0.27707118,-0.34634376,0.33794892,-0.059091955,-0.0902479,0.15171182,-0.20944837,0.37704355]  |[[-5.0], [-6.0], [-1.0]]|\n",
      "|113    |[1.1042031,0.6060762,0.14310445,0.02224311,0.30606657,7.456416E-6,0.11599636,0.21357717,0.15067935,0.2607651]        |[[-3.0], [-5.0], [0.0]] |\n",
      "|34     |[-0.16783164,0.36099303,0.15585437,-0.037834037,-0.5871588,-0.16798319,-0.02284482,0.1691373,-0.0108423475,0.2633466]|[[1.0], [3.0], [-2.0]]  |\n",
      "+-------+---------------------------------------------------------------------------------------------------------------------+------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Approximately searching for 5 nearest neighbors of the sample embedding:\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|movieId|emb                                                                                                              |bucketId               |distCol           |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "|836    |[1.0528573,0.33895427,0.6294386,0.13048036,0.43996274,0.028281,-0.1980697,0.46195424,0.48402253,-0.05034756]     |[[-9.0], [-5.0], [1.0]]|1.3569761290962867|\n",
      "|153    |[-0.20721631,0.50782675,0.95114654,0.3285679,0.40620616,-0.5975689,0.45902735,-0.19455203,0.14672351,-0.19017138]|[[-5.0], [0.0], [-2.0]]|1.4735753389806998|\n",
      "|414    |[0.26897326,0.694944,0.4227652,-0.082158364,0.29195687,-0.24629378,0.23407763,0.4281068,0.44660276,0.060402595]  |[[-3.0], [0.0], [1.0]] |1.5065597807810742|\n",
      "|748    |[0.9622548,0.062694885,0.5309618,0.09276987,0.42129502,0.18462588,-0.03265658,0.69441384,0.4942232,0.1301593]    |[[-9.0], [-5.0], [3.0]]|1.641377295026164 |\n",
      "|71     |[0.72963315,-0.10944478,0.2966183,-0.1419375,0.4283504,-0.22018869,0.13182826,0.07298865,0.15585506,-0.12607768] |[[-6.0], [-7.0], [3.0]]|1.7152461260165035|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------+-----------------------+------------------+\n",
      "\n",
      "Train User Embedding...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    # Change to your own filepath\n",
    "    file_path = '../../data/'\n",
    "    rawSampleDataPath = file_path + \"ratings.csv\"\n",
    "    embLength = 10\n",
    "    print(\"Process ItemSquence...\")\n",
    "    samples = processItemSequence(spark, rawSampleDataPath)\n",
    "    print(\"Train Item2Vec...\")\n",
    "    model = trainItem2vec(spark, samples, embLength,\n",
    "                          embOutputPath=file_path + \"modeldata/item2vecEmb.csv\", saveToRedis=False,\n",
    "                          redisKeyPrefix=\"i2vEmb\")\n",
    "    print(\"Train graph Embedding...\")\n",
    "    graphEmb(samples, spark, embLength, embOutputFilename=file_path + \"modeldata/itemGraphEmb.csv\",\n",
    "             saveToRedis=True, redisKeyPrefix=\"graphEmb\")\n",
    "    print(\"Train User Embedding...\")\n",
    "    generateUserEmb(spark, rawSampleDataPath, model, embLength,\n",
    "                    embOutputPath=file_path + \"modeldata/userEmb.csv\", saveToRedis=False,\n",
    "                    redisKeyPrefix=\"uEmb\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing embedding.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile embedding.py\n",
    "\n",
    "def getItemSeqs(spark, samplesRating):\n",
    "    \"\"\"\n",
    "    extract item sequences for each user from dataframe\n",
    "    1. for each user, collect the corresponding visited movies and timestamp into a list\n",
    "    2. use UDF to process movie list and timestamp list to sort the movie sequence for each user\n",
    "    3. join the movie list to get a string for each user\n",
    "    \"\"\"\n",
    "    def sortF(movie_list, timestamp_list):\n",
    "        \"\"\"\n",
    "        sort by time and return the corresponding movie sequence\n",
    "        eg:\n",
    "            input: movie_list:[1,2,3]\n",
    "                   timestamp_list:[1112486027,1212546032,1012486033]\n",
    "            return [3,1,2]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        # concat timestamp with movie id\n",
    "        for m, t in zip(movie_list, timestamp_list):\n",
    "            pairs.append((m, t))\n",
    "        # sort by time\n",
    "        pairs = sorted(pairs, key=lambda x: x[1])\n",
    "        return [x[0] for x in pairs]\n",
    "    \n",
    "    \n",
    "    sortUDF = udf(sortF, ArrayType(StringType()))\n",
    "    \n",
    "    # rating data\n",
    "    #ratingSamples.show(5)\n",
    "    # ratingSamples.printSchema()\n",
    "    userSequence = samplesRating.where(F.col(\"rating\") > 3) \\\n",
    "                    .groupBy(\"userId\")\\\n",
    "                    .agg(sortUDF(F.collect_list(\"movieId\"), F.collect_list(\"timestamp\")).alias(\"movieIds\"))\\\n",
    "                    .withColumn(\"movieIdStr\", F.array_join(F.col(\"movieIds\"), \" \"))\n",
    "    seq = userSequence.select(\"movieIdStr\").rdd.map(lambda x : x[0].split(\" \"))\n",
    "    #print(seq.collect()[:5])\n",
    "    return seq\n",
    "\n",
    "\n",
    "\n",
    "def embeddingLSH(spark, movieEmbMap):\n",
    "    \"\"\"\n",
    "    Local sensitive hashing using bucketedRandomProjection\n",
    "    \"\"\"\n",
    "    movieEmbSeq = []\n",
    "    for key, embedding_list in movieEmbMap.items():\n",
    "        embedding_list = [np.float64(embedding) for embedding in embedding_list]\n",
    "        movieEmbSeq.append((key, Vectors.dense(embedding_list)))\n",
    "    movieEmbDF = spark.createDataFrame(movieEmbSeq).toDF(\"movieId\", \"emb\")\n",
    "    bucketProjectionLSH = BucketedRandomProjectionLSH(inputCol=\"emb\", outputCol=\"bucketId\", bucketLength=0.1,\n",
    "                                                      numHashTables=3)\n",
    "    bucketModel = bucketProjectionLSH.fit(movieEmbDF)\n",
    "    embBucketResult = bucketModel.transform(movieEmbDF)\n",
    "    print(\"movieId, emb, bucketId schema:\")\n",
    "    embBucketResult.printSchema()\n",
    "    print(\"movieId, emb, bucketId data result:\")\n",
    "    embBucketResult.show(10, truncate=False)\n",
    "    print(\"Approximately searching for 5 nearest neighbors of the sample embedding:\")\n",
    "    sampleEmb = Vectors.dense(0.795, 0.583, 1.120, 0.850, 0.174, -0.839, -0.0633, 0.249, 0.673, -0.237)\n",
    "    bucketModel.approxNearestNeighbors(movieEmbDF, sampleEmb, 5).show(truncate=False)\n",
    "\n",
    "\n",
    "\n",
    "def getTransitionMatrix(item_seq):\n",
    "    \"\"\"\n",
    "    build graph and transition matrix based on input item sequences \n",
    "    input: list of item sequence in RDD format\n",
    "    output: transition matrix and item distribution in dictionary format\n",
    "    \n",
    "    \"\"\"\n",
    "    def generate_pair(x):\n",
    "        \"\"\"\n",
    "        use a sliding window with size of 2 to generate item pairs\n",
    "        input: ls =  list of items \n",
    "        output: list of pairs\n",
    "        example:\n",
    "            input: [86, 90, 11, 100,]\n",
    "            output: [[86,90], [90, 11], [11,100]]\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        prev = None\n",
    "        print(x)\n",
    "        for i in range(len(x)):\n",
    "            if i >0:\n",
    "                res.append((x[i-1],x[i]))\n",
    "        return res\n",
    "\n",
    "    \n",
    "    #  convert item sequences to pair list \n",
    "    pair_seq = item_seq.flatMap(lambda x: generate_pair(x))\n",
    "    # convert pair list to  dictionary, key = pair, value = count\n",
    "    pair_count_dict = pair_seq.countByValue()\n",
    "    tot_count = pair_seq.count()\n",
    "    trans_matrix =  defaultdict(dict)\n",
    "    item_count = defaultdict(int)\n",
    "    item_dist = defaultdict(float)\n",
    "    \n",
    "    # consider out-degree only \n",
    "    for item, cnt in pair_count_dict.items():\n",
    "        item1, item2 = item[0], item[1]\n",
    "        item_count[item1] +=  cnt\n",
    "        trans_matrix[item1][item2] = cnt\n",
    "        \n",
    "    for item, cnt in pair_count_dict.items():\n",
    "        item1, item2 = item[0], item[1]\n",
    "        # possibility of transition\n",
    "        trans_matrix[item1][item2] /= item_count[item1]\n",
    "        # distribution of each source node (item)\n",
    "        item_dist[item1] =  item_count[item1]/tot_count\n",
    "        \n",
    "    return trans_matrix, item_dist\n",
    "\n",
    "def oneRandomWalk(trans_mat, item_dist, sample_length):\n",
    "    \"\"\"\n",
    "    generate one random walk sequence based on transition matrix\n",
    "    input: \n",
    "        - trans_mat: transition matrix\n",
    "        - item_dist: distribution of item\n",
    "        - sample length: number of node in a path  = length of a walk -1 = length of edges - 1\n",
    "    \"\"\"\n",
    "    rand_val = random.random()\n",
    "    # randomly pick item based on CDF , cumulative density function, obtained from the item distribution\n",
    "    # we can also randomly pick a item based on the distribution using  choice () function from numpy as well\n",
    "    cdf_prob =0\n",
    "    first_item = ''\n",
    "    for item, prob in item_dist.items():\n",
    "        cdf_prob += prob\n",
    "        if cdf_prob >= rand_val:\n",
    "            first_item = item\n",
    "            break\n",
    "    item_list = [first_item]\n",
    "    cur_item = first_item\n",
    "    \n",
    "    while len(item_list) < sample_length:\n",
    "        if (cur_item not in item_dist) or (cur_item not in trans_mat):\n",
    "            break\n",
    "        cdf_prob = 0\n",
    "        rand_val = random.random()\n",
    "        dist = trans_mat[cur_item]\n",
    "        for item, prob in dist.items():\n",
    "            cdf_prob += prob\n",
    "            if cdf_prob >= rand_val:\n",
    "                cur_item = item\n",
    "                break\n",
    "        item_list.append(cur_item)\n",
    "        \n",
    "    return item_list\n",
    "\n",
    "\n",
    "\n",
    "def generateItemSeqs(trans_mat, item_dist, num_seq=20000, sample_length = 10  ):\n",
    "    \"\"\"\n",
    "    use random walk to generate multiple item sequences\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for i in range(num_seq):\n",
    "        samples.append(oneRandomWalk(trans_mat, item_dist, sample_length))\n",
    "    \n",
    "    return samples\n",
    "\n",
    "\n",
    "def trainItem2Vec(spark, item_seqs, emb_length, output_path, save_to_redis=False, redis_keyprefix=None):\n",
    "    \"\"\"\n",
    "    use Word2Vec to train item embedding\n",
    "    input:\n",
    "        - item_seqs: RDD pipeline instance, rather than dataframe\n",
    "    Note:  \n",
    "    - Word2Vec from mllib is a function that take RDD pipeline as input.\n",
    "    - Word2Vec from ml is a function that take Dataframe as input \n",
    "    \n",
    "    \"\"\"\n",
    "    # train word2Vec\n",
    "#     w2v = Word2Vec(vectorSize=emb_length, windowSize = 5, maxIter = 10, seed=42)\n",
    "    w2v = Word2Vec().setVectorSize(embLength).setWindowSize(5).setNumIterations(10)\n",
    "    model = w2v.fit(item_seqs)\n",
    "    # test word2vec\n",
    "    synonyms = model.findSynonyms(\"157\", 20)\n",
    "    for synonym, cos_similarity in synonyms:\n",
    "        print(synonym, cos_similarity)\n",
    "    \n",
    "    # save word2Vec to input path \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    with open(output_path, \"w\") as fp:\n",
    "        for movie_id in model.getVectors():\n",
    "            # convert vector to string type and store it\n",
    "            vector = \" \".join([str(emb) for emb in model.getVectors()[movie_id]])\n",
    "            pair = movie_id + \":\" + vector + \"\\n\"\n",
    "            fp.write(pair)\n",
    "    return model\n",
    "\n",
    "\n",
    "def getDeepWalk(spark, item_seq, sample_length=10, num_walk=20000, output_file='../../data/modeldata/embedding.csv',\n",
    "             save_to_redis=False, redis_key_prefix=None):\n",
    "    \"\"\"\n",
    "    use DeepWalk to generate graph embeddings\n",
    "    input:\n",
    "        - item_seq: RDD based sequence of item visited by a user\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # construct probability graph\n",
    "    trans_mat, item_dist = getTransitionMatrix(item_seq)\n",
    "    \n",
    "    # generate sequence samples randomly\n",
    "    samples = generateItemSeqs(trans_mat, item_dist,num_seq=num_walk, sample_length = sample_length )\n",
    "    # convert list of samples to spark rdd \n",
    "    samples_rdd = spark.sparkContext.parallelize(samples)\n",
    "    # train item2Vec\n",
    "    graphEmbModel = trainItem2Vec(spark, samples_rdd, emb_length=10, output_path=output_file , save_to_redis=False, redis_keyprefix=None)\n",
    "    \n",
    "    return graphEmbModel\n",
    "\n",
    "def getUserEmb( spark ,samples_rating, item_emb_model, output_file):\n",
    "    \"\"\"\n",
    "    generate user embedding based on item embedding\n",
    "    use map reduce to sum up embeddings of items purchased by user to generate user embedding\n",
    "    input:\n",
    "        - spark: spark session\n",
    "        - samples_rating: dataframe with rating, movieId, userId data\n",
    "        - item_emb_model: word2Vec/Item2Vec model trained by deep walk. \n",
    "        - output_file: file name of user embedding \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "#     assert not item_emb or not item_emb_path, \"Must input either item embedding vectors or path\"\n",
    "#     if item_emb_path != None:\n",
    "#         item_emb = spark.read.csv(item_emb_path, header=True)\n",
    "\n",
    "    emb_dict = item_emb_model.getVectors()\n",
    "    item_emb_ls=[]\n",
    "    for item, emb in emb_dict.items():\n",
    "        #print((item, emb))\n",
    "        item_emb_ls.append((item, list(emb)))\n",
    "    fields = [StructField('movieId', StringType(),False),\n",
    "             StructField('emb', ArrayType(FloatType()),False),]\n",
    "    item_emb_schema = StructType(fields)\n",
    "    item_emb_df = spark.createDataFrame(item_emb_ls, item_emb_schema)\n",
    "    \n",
    "    # apply mapreduce to sum up item embeddings for each user to obtain user embedding\n",
    "    # Note: we need inner join here to avoid empty item embedding during mapreduce calculation\n",
    "    user_emb = samples_rating.join(item_emb_df, on=\"movieId\", how=\"inner\")\n",
    "    print()\n",
    "    print(\"User Embdding\")\n",
    "    user_emb.show(5)\n",
    "    user_emb.printSchema()\n",
    "    user_emb = user_emb.select(\"userId\",\"emb\").rdd.map(lambda row: (row[0], row[1]) ).reduceByKey(lambda emb1, emb2: [ float(emb1[i]) + float(emb2[i]) for i in range(len(emb1))] ).collect()\n",
    "    print(user_emb[:5])\n",
    "    #save user embedding\n",
    "    with open(output_file,\"w\") as fp:\n",
    "        for userId, emb in user_emb:\n",
    "            row = \" \".join([str(e) for e in emb])\n",
    "            row = str(userId)+ \":\"+ row + \"\\n\"\n",
    "            fp.write(row)\n",
    "    print(\"User Embedding Saved!\")\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    conf = SparkConf().setAppName('ctrModel').setMaster('local')\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    # Change to your own filepath\n",
    "    file_path = '../../data/'\n",
    "    rawSampleDataPath = file_path + \"ratings.csv\"\n",
    "    embLength = 10\n",
    "    print(\"Process ItemSquence...\")\n",
    "    samplesRating = spark.read.csv(rawSampleDataPath, header = True)\n",
    "    item_seqs = getItemSeqs(spark, samplesRating)\n",
    "    #print(samples)\n",
    "    \n",
    "    #trainItem2Vec(item_seqs, emb_length=10, output_path=file_path+\"modeldata/itemGraphEmb.csv\", save_to_redis=False, redis_keyprefix=None)\n",
    "    \n",
    "    graphEmb = getDeepWalk(spark, item_seqs, sample_length=10, num_walk=20000, output_file=file_path+\"modeldata/itemGraphEmb.csv\",\n",
    "             save_to_redis=False, redis_key_prefix=None)\n",
    "    getUserEmb( spark ,samples_rating= samplesRating, item_emb_model= graphEmb, output_file= file_path+\"modeldata/userEmb.csv\")\n",
    "\n",
    "    print(\"Done!\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "+ **Difference between mllib and ml in pyspark**\n",
    "    - pyspark.mllib library is based on RDD \n",
    "    - pyspark.ml is based on Spark DataFrame. Spark DataFrame is also based on RDD, so it  can be converted to RDD as well\n",
    "+  **Useful functions**\n",
    "    - rdd.map():  make rdd data with length of N to N collections with length of N:  ['ab','' ,'cd''' ,'ef'] -> [['ab'],[], ['cd'], []['ef']]\n",
    "    - rdd.flatMap(): make rdd data with length of N to one collection: ['ab','' ,'cd''' ,'ef'] -> ['ab','cd' ,'ef']\n",
    "    - spark.sparkContext().parallelize() is equivalen to SparkContext().parallelize(), but be careful about the first charater \"sparkContext\" and \"SparkContext\"\n",
    "    - SparkContext().parallelize(c= [....], numSlices= n): convert python collection to RDD with slices = numSlices\n",
    "    \n",
    "+ **MapReduce in Spark**\n",
    "    - dataframe.select(\"value1\",\"value2\",\"value3\"...).rdd.map(map_func()): \n",
    "        + **Input to mapper function is  RDD Row() datatype. We can use lambda a: (a[0], a[1],..) to convert it to tuples**\n",
    "        + In rdd.map(..), we can apply a mapper function to map and transform the dataframe to different format\n",
    "    - dataframe.select(\"value1\",\"value2\",\"value3\"...).rdd.map(map_func()).reduceByKey(reduce_func())\n",
    "        + **Input to reduce function is either an aggregation function from PySpark or UDF with 2 inputs, representing 2 rows in dataframe with the same key**\n",
    "        + If we want to apply reduceByKey() after mapper function, we need to use map(lambda a: (a[0], a[1],..)) to convert Row() format to list of tuple. **The first column from mapper function is used as key in reduceByKey function**\n",
    "        \n",
    "        + Example: \n",
    "            - df.select(\"value1\",\"value2\",\"value3\").rdd.map(lambda a: (a[0], a[1], a[2])).reduceByKey(lambda a,b: a+b ).\n",
    "            - It first convert ROW() to list of tuple (a[0], a[1], a[2]) with **the first column a[0] as key**\n",
    "            - reduceByKey(lambda a,b: a+b ) is to add two values from row a and row b to perform reduce operation\n",
    "+ **Common Issues**\n",
    "    - Usually when the usage and syntax of spark query, like map reduce are correct, but some issues occurs. There are  some possible reasons:\n",
    "    \n",
    "    - **None value exists during computing**, leading to calculation error\n",
    "    - **Data types** between schema and input data don't match\n",
    "    - SparkSession doesn't start before computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv_v2",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
